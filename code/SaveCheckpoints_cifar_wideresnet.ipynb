{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SaveCheckpoints_cifar_wideresnet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b444023624bd44c4b08e9f780c6b1473":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_43d949843c734f6e9d2ef24726667c26","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c7205b957e724c3ebdb4a161622879f2","IPY_MODEL_a6ad4050a4a340b893b87f4d77f00e2c"]}},"43d949843c734f6e9d2ef24726667c26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7205b957e724c3ebdb4a161622879f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5a7d54ea535440cba02070a23d4b9a43","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4cc3959c07a340cdacd528b68db8572d"}},"a6ad4050a4a340b893b87f4d77f00e2c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_27128ec958e24271a8fe0e01235c222b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:06&lt;00:00,  6.63s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36126a58f19c4baf8bbf9fe6beb47465"}},"5a7d54ea535440cba02070a23d4b9a43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4cc3959c07a340cdacd528b68db8572d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27128ec958e24271a8fe0e01235c222b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"36126a58f19c4baf8bbf9fe6beb47465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b2424d926114aae99917aeee01405dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4eaa57c3ff494b34949d70ec4becdcf9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6b2d6c76730f43e0b727f4d13f7ba473","IPY_MODEL_cacc21054f3a40f28df5ad2663615798"]}},"4eaa57c3ff494b34949d70ec4becdcf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b2d6c76730f43e0b727f4d13f7ba473":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6bdf10bd24894457bcd4483afc8aeedc","_dom_classes":[],"description":"Dl Size...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d5f64c5b5ed4fd7aae5c5611b6abab5"}},"cacc21054f3a40f28df5ad2663615798":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_60cb783686884e70a893c8e71853445f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 162/162 [00:06&lt;00:00, 24.75 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9f0644d5cbc14328914e1aed3e82da19"}},"6bdf10bd24894457bcd4483afc8aeedc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8d5f64c5b5ed4fd7aae5c5611b6abab5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"60cb783686884e70a893c8e71853445f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9f0644d5cbc14328914e1aed3e82da19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1fe5b02a125542e5b20bd45fc8251fea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d7190eeb37a04346893a500e67b544bd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f51eeb4751eb4709b22ba7e089329755","IPY_MODEL_18e6497287964684a0964d6249d8504a"]}},"d7190eeb37a04346893a500e67b544bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f51eeb4751eb4709b22ba7e089329755":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d0876294fc2548e3bf8fc48d16eeeb47","_dom_classes":[],"description":"Extraction completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_159b7175271945f1ae5ecec70830f378"}},"18e6497287964684a0964d6249d8504a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_241c5816527f427690769335f894c244","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:06&lt;00:00,  6.48s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8ffc1c827cd7415f8bfa063734388580"}},"d0876294fc2548e3bf8fc48d16eeeb47":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"159b7175271945f1ae5ecec70830f378":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"241c5816527f427690769335f894c244":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8ffc1c827cd7415f8bfa063734388580":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b2ed2895b8d414f99dbd935ee05789b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7730ea5a80c54d1d88e4a704c9aaea0c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_42c1b5191b524384a5cde2a12652bf17","IPY_MODEL_87f09001b8a54666a0e12adc01d5612e"]}},"7730ea5a80c54d1d88e4a704c9aaea0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42c1b5191b524384a5cde2a12652bf17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_af345b9e265f43aa8e1c561e8fa2bcf4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5d218185109e446ca1fc6c4b5030b911"}},"87f09001b8a54666a0e12adc01d5612e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_39fbbbcaff044d52b83d0363529ed70b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 50000/0 [00:34&lt;00:00, 1521.40 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ab5b07e5eda4d99968663f250972b3f"}},"af345b9e265f43aa8e1c561e8fa2bcf4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5d218185109e446ca1fc6c4b5030b911":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39fbbbcaff044d52b83d0363529ed70b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3ab5b07e5eda4d99968663f250972b3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6a6b6aa1e17498fbb4e6c227f2c1e60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_32a78bbb459d45df9e68db657d1f1e8a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_832e069a842e46dcb818a077c4c13cdf","IPY_MODEL_96508d49248f4282b27c9dd53c946075"]}},"32a78bbb459d45df9e68db657d1f1e8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"832e069a842e46dcb818a077c4c13cdf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c811c7a653de4d049170ad09b3ae7745","_dom_classes":[],"description":" 87%","_model_name":"FloatProgressModel","bar_style":"danger","max":50000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":43741,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_219eb0c2ea04426284fecd5828718d92"}},"96508d49248f4282b27c9dd53c946075":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0b6c96fc37d4664bc19c23cc58e8d76","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 43741/50000 [00:00&lt;04:09, 25.06 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19de116dd89c4c1ab8ffc7bc6adf6ee8"}},"c811c7a653de4d049170ad09b3ae7745":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"219eb0c2ea04426284fecd5828718d92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0b6c96fc37d4664bc19c23cc58e8d76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"19de116dd89c4c1ab8ffc7bc6adf6ee8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8c269ceb8ad44ee9a0af007226ad376":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_99e9d94bd69d4428b19acac9cc473550","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6d064b0b31ec45acbac0c111225345ba","IPY_MODEL_7e43d76cd25244338ffbae5edae72353"]}},"99e9d94bd69d4428b19acac9cc473550":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d064b0b31ec45acbac0c111225345ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4a7ec57a8c564af1851c76a99c2d2228","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_789ab1c3f09a48f9930f03ccd728d45b"}},"7e43d76cd25244338ffbae5edae72353":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ad0aeda093dd4871aca71c3f2918e3a2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10000/0 [00:06&lt;00:00, 1450.83 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae92f2d5706e45389cd8faf43af500f4"}},"4a7ec57a8c564af1851c76a99c2d2228":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"789ab1c3f09a48f9930f03ccd728d45b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad0aeda093dd4871aca71c3f2918e3a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ae92f2d5706e45389cd8faf43af500f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"492b6fb9f1dd49338cfc853d0fa60895":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_742a0ad37b304f25ba38d3854f05501a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5188eb145a104021aafad581bdb1f690","IPY_MODEL_e26ee93000ae4de88b2893b0c96cc560"]}},"742a0ad37b304f25ba38d3854f05501a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5188eb145a104021aafad581bdb1f690":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f27371e2c9e34479997c317deafef629","_dom_classes":[],"description":" 53%","_model_name":"FloatProgressModel","bar_style":"danger","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5287,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38abbfd97ac842bbb0ea2e3adc47c13b"}},"e26ee93000ae4de88b2893b0c96cc560":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a8ddb633aad4e678edd3d04d1f9e56d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5287/10000 [00:20&lt;00:00, 52867.03 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9b0356a2b86d4e1a8527f6f756611d0b"}},"f27371e2c9e34479997c317deafef629":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"38abbfd97ac842bbb0ea2e3adc47c13b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a8ddb633aad4e678edd3d04d1f9e56d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9b0356a2b86d4e1a8527f6f756611d0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVpBmTS2QxV6","executionInfo":{"status":"ok","timestamp":1623391522169,"user_tz":-540,"elapsed":72411,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"924f750c-aad3-4b7d-87d9-39d13813a789"},"source":["!pip install \"git+https://github.com/google/uncertainty-baselines.git#egg=uncertainty_baselines\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting uncertainty_baselines\n","  Cloning https://github.com/google/uncertainty-baselines.git to /tmp/pip-install-s1nxq_ap/uncertainty-baselines\n","  Running command git clone -q https://github.com/google/uncertainty-baselines.git /tmp/pip-install-s1nxq_ap/uncertainty-baselines\n","Requirement already satisfied: absl-py>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (0.12.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.19.5)\n","Collecting tb-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/05/451e0103da806582df3beba4055cafb16a943f5eda3b2fd56c199284d3f4/tb_nightly-2.6.0a20210609-py3-none-any.whl (5.5MB)\n","\u001b[K     |████████████████████████████████| 5.5MB 5.3MB/s \n","\u001b[?25hCollecting tf-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/d5/a2189a5e9f51f1e1eeff9466d75a194d6b875e9a4e700f24ebf58bf0a6ee/tf_nightly-2.6.0.dev20210610-cp37-cp37m-manylinux2010_x86_64.whl (454.9MB)\n","\u001b[K     |████████████████████████████████| 454.9MB 28kB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (4.0.1)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.6.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (3.3.0)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.12)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (3.4.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (1.24.3)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (3.0.4)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from uncertainty_baselines) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.8.1->uncertainty_baselines) (1.15.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.34.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (3.12.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (57.0.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (0.36.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (0.4.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.30.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->uncertainty_baselines) (0.6.1)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (3.1.0)\n","Collecting tf-estimator-nightly~=2.5.0.dev\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/6c/9bf4a6004d18c8e543845d3416e50f36dd09d272161e2fb0db5678132dfd/tf_estimator_nightly-2.5.0.dev2021032601-py2.py3-none-any.whl (462kB)\n","\u001b[K     |████████████████████████████████| 471kB 32.8MB/s \n","\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (1.1.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (1.12.1)\n","Collecting keras-nightly~=2.6.0.dev\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/42/349c84c490f1a6b5a283f9805be1e9e865f9dc0b0345742d72b549058757/keras_nightly-2.6.0.dev2021061000-py2.py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 35.8MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (0.2.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (1.1.2)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (3.7.4.3)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->uncertainty_baselines) (0.4.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (0.3.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (1.0.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (2.3)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (5.1.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (4.41.1)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (21.2.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->uncertainty_baselines) (0.1.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly->uncertainty_baselines) (2020.12.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->uncertainty_baselines) (4.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->uncertainty_baselines) (1.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (4.7.2)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tf-nightly->uncertainty_baselines) (1.5.2)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->uncertainty_baselines) (1.53.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->uncertainty_baselines) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly->uncertainty_baselines) (0.4.8)\n","Building wheels for collected packages: uncertainty-baselines\n","  Building wheel for uncertainty-baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for uncertainty-baselines: filename=uncertainty_baselines-0.0.7-cp37-none-any.whl size=276124 sha256=f97f4610a4356880e8d2de791b6e2805f90e0202959c682cae141243c31f1adf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-7ik3nnqg/wheels/5b/77/8b/8954f4644619426772bf3f47cb2246de4be5409186b43b0264\n","Successfully built uncertainty-baselines\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement keras-nightly~=2.5.0.dev, but you'll have keras-nightly 2.6.0.dev2021061000 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tf-nightly 2.6.0.dev20210610 has requirement grpcio<2.0,>=1.37.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n","Installing collected packages: tb-nightly, tf-estimator-nightly, keras-nightly, tf-nightly, uncertainty-baselines\n","  Found existing installation: keras-nightly 2.5.0.dev2021032900\n","    Uninstalling keras-nightly-2.5.0.dev2021032900:\n","      Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n","Successfully installed keras-nightly-2.6.0.dev2021061000 tb-nightly-2.6.0a20210609 tf-estimator-nightly-2.5.0.dev2021032601 tf-nightly-2.6.0.dev20210610 uncertainty-baselines-0.0.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrsFLW26bXJY","executionInfo":{"status":"ok","timestamp":1623391534192,"user_tz":-540,"elapsed":12028,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"5aa06439-5a6b-44ee-ca51-e5278965bfda"},"source":["pip install \"git+https://github.com/google-research/robustness_metrics.git#egg=robustness_metrics\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting robustness_metrics\n","  Cloning https://github.com/google-research/robustness_metrics.git to /tmp/pip-install-wlryskt3/robustness-metrics\n","  Running command git clone -q https://github.com/google-research/robustness_metrics.git /tmp/pip-install-wlryskt3/robustness-metrics\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.12.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (1.1.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.22.2.post1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.8.9)\n","Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (4.0.1)\n","Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (0.12.0)\n","Requirement already satisfied: tf-nightly in /usr/local/lib/python3.7/dist-packages (from robustness_metrics) (2.6.0.dev20210610)\n","Collecting tfp-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/bf/5c10b08b2fa414ecb81ada5ee31decba706f4aefd052b2819fd405c7222e/tfp_nightly-0.14.0.dev20210610-py2.py3-none-any.whl (5.5MB)\n","\u001b[K     |████████████████████████████████| 5.5MB 5.4MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->robustness_metrics) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->robustness_metrics) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->robustness_metrics) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->robustness_metrics) (1.19.5)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->robustness_metrics) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->robustness_metrics) (1.0.1)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (5.1.3)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (0.1.6)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (0.3.3)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (1.1.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (1.0.0)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (21.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (4.41.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (0.16.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (3.12.4)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (2.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->robustness_metrics) (2.23.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (3.7.4.3)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (3.3.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (0.4.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.12.1)\n","Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (2.5.0.dev2021032601)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.12)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.6.3)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (0.36.2)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (0.2.0)\n","Collecting grpcio<2.0,>=1.37.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/af/ecae3a21bed5a92c9d866480553efea18a39f103546108cd60f5ea6a2494/grpcio-1.38.0-cp37-cp37m-manylinux2014_x86_64.whl (4.2MB)\n","\u001b[K     |████████████████████████████████| 4.2MB 37.5MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (1.1.2)\n","Requirement already satisfied: keras-nightly~=2.6.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (2.6.0.dev2021061000)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (3.1.0)\n","Requirement already satisfied: tb-nightly~=2.6.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly->robustness_metrics) (2.6.0a20210609)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tfp-nightly->robustness_metrics) (4.4.2)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tfp-nightly->robustness_metrics) (1.3.0)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets->robustness_metrics) (3.4.1)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->robustness_metrics) (1.53.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow_datasets->robustness_metrics) (57.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets->robustness_metrics) (2020.12.5)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tf-nightly->robustness_metrics) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.6.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.30.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.4.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.8.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (4.7.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (4.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly->robustness_metrics) (3.1.0)\n","Building wheels for collected packages: robustness-metrics\n","  Building wheel for robustness-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for robustness-metrics: filename=robustness_metrics-0.0.1-cp37-none-any.whl size=99579 sha256=721a759166ea233e26df91c14b40e3e207a7ae94eda01f3d9dce4f383b250d93\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-39gf0x3x/wheels/f1/09/d6/441f73c886118e9f7afb6c7c15f0273279567036d7390e345d\n","Successfully built robustness-metrics\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement grpcio~=1.34.0, but you'll have grpcio 1.38.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement keras-nightly~=2.5.0.dev, but you'll have keras-nightly 2.6.0.dev2021061000 which is incompatible.\u001b[0m\n","Installing collected packages: tfp-nightly, robustness-metrics, grpcio\n","  Found existing installation: grpcio 1.34.1\n","    Uninstalling grpcio-1.34.1:\n","      Successfully uninstalled grpcio-1.34.1\n","Successfully installed grpcio-1.38.0 robustness-metrics-0.0.1 tfp-nightly-0.14.0.dev20210610\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tUhcLEebZny","executionInfo":{"status":"ok","timestamp":1623391540475,"user_tz":-540,"elapsed":6297,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"6ac6c051-9a64-4159-c45e-dd93470d0e65"},"source":["pip install \"git+https://github.com/google/edward2\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/google/edward2\n","  Cloning https://github.com/google/edward2 to /tmp/pip-req-build-qfybt_4a\n","  Running command git clone -q https://github.com/google/edward2 /tmp/pip-req-build-qfybt_4a\n","Building wheels for collected packages: edward2\n","  Building wheel for edward2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for edward2: filename=edward2-0.0.3-cp37-none-any.whl size=168433 sha256=4e0f1f7e1af8363b713ba3a621c77cf075480a9d36ab67cc6928ce17143f3965\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-p_jetlox/wheels/f9/fb/ed/96a1b2e305c0df592e14b8a7bf576b09f1540542771856eb15\n","Successfully built edward2\n","Installing collected packages: edward2\n","Successfully installed edward2-0.0.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nY10M9WKNwBB"},"source":["#시작"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CoeNznmInaA","executionInfo":{"status":"ok","timestamp":1623391614867,"user_tz":-540,"elapsed":74403,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"0a222b56-5f30-44d4-e57a-da742fc14121"},"source":["# 구글 드라이브 마운트\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"dhzxWl_2Ip1D","executionInfo":{"status":"error","timestamp":1623248911362,"user_tz":-540,"elapsed":488,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"2ed21fe2-34f4-4e1e-a167-a532626335a3"},"source":["# import pandas as pd\n","# df = pd.read_csv('/content/gdrive/My Drive/tmp/cifar')\n","# print(len(df))\n","# df.head()"],"execution_count":5,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-be3e548e2e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/tmp/cifar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."]}]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":76},"id":"P-W9SXgTOA1u","executionInfo":{"status":"ok","timestamp":1623391639267,"user_tz":-540,"elapsed":9986,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"3ad7ceeb-aa3a-4158-e7ad-44aa46481b83"},"source":["from google.colab import files\n","src = list(files.upload().values())[0]\n","open('utils.py','wb').write(src)\n","import utils"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-dfb8bd9e-a6cf-4934-b446-00d1355f916e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-dfb8bd9e-a6cf-4934-b446-00d1355f916e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving utils.py to utils.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5oRio5jMcvJW"},"source":["# cifar 데이터셋"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6lISZ6rZ43e","executionInfo":{"status":"ok","timestamp":1623391644783,"user_tz":-540,"elapsed":5518,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"55cb9f74-d6d7-450d-e069-afcf365144b0"},"source":["#cifar데이터셋\n","\n","# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","\"\"\"CIFAR{10,100} dataset builders.\"\"\"\n","\n","from typing import Any, Dict, Optional, Union\n","\n","from robustness_metrics.common import types\n","import tensorflow.compat.v2 as tf\n","import tensorflow_datasets as tfds\n","from uncertainty_baselines.datasets import augment_utils\n","from uncertainty_baselines.datasets import augmix\n","from uncertainty_baselines.datasets import base\n","\n","# We use the convention of using mean = np.mean(train_images, axis=(0,1,2))\n","# and std = np.std(train_images, axis=(0,1,2)).\n","CIFAR10_MEAN = tf.constant([0.4914, 0.4822, 0.4465])\n","CIFAR10_STD = tf.constant([0.2470, 0.2435, 0.2616])\n","# Previously we used std = np.mean(np.std(train_images, axis=(1, 2)), axis=0)\n","# which gave std = tf.constant([0.2023, 0.1994, 0.2010], dtype=dtype), however\n","# we change convention to use the std over the entire training set instead.\n","\n","\n","def _tuple_dict_fn_converter(fn, *args):\n","\n","  def dict_fn(batch_dict):\n","    images, labels = fn(*args, batch_dict['features'], batch_dict['labels'])\n","    return {'features': images, 'labels': labels}\n","\n","  return dict_fn\n","\n","\n","class _CifarDataset(base.BaseDataset):\n","  \"\"\"CIFAR dataset builder abstract class.\"\"\"\n","\n","  def __init__(\n","      self,\n","      name: str,\n","      fingerprint_key: str,\n","      split: str,\n","      seed: Optional[Union[int, tf.Tensor]] = None,\n","      validation_percent: float = 0.0,\n","      shuffle_buffer_size: Optional[int] = None,\n","      num_parallel_parser_calls: int = 64,\n","      drop_remainder: bool = True,\n","      normalize: bool = True,\n","      try_gcs: bool = False,\n","      download_data: bool = False,\n","      use_bfloat16: bool = False,\n","      aug_params: Optional[Dict[str, Any]] = None,\n","      data_dir: Optional[str] = None,\n","      is_training: Optional[bool] = None,\n","      **unused_kwargs: Dict[str, Any]):\n","    \"\"\"Create a CIFAR10 or CIFAR100 tf.data.Dataset builder.\n","    Args:\n","      name: the name of this dataset, either 'cifar10' or 'cifar100'.\n","      fingerprint_key: The name of the feature holding a string that will be\n","        used to create an element id using a fingerprinting function. If None,\n","        then `ds.enumerate()` is added before the `ds.map(preprocessing_fn)` is\n","        called and an `id` field is added to the example Dict.\n","      split: a dataset split, either a custom tfds.Split or one of the\n","        tfds.Split enums [TRAIN, VALIDAITON, TEST] or their lowercase string\n","        names.\n","      seed: the seed used as a source of randomness.\n","      validation_percent: the percent of the training set to use as a validation\n","        set.\n","      shuffle_buffer_size: the number of example to use in the shuffle buffer\n","        for tf.data.Dataset.shuffle().\n","      num_parallel_parser_calls: the number of parallel threads to use while\n","        preprocessing in tf.data.Dataset.map().\n","      drop_remainder: whether or not to drop the last batch of data if the\n","        number of points is not exactly equal to the batch size. This option\n","        needs to be True for running on TPUs.\n","      normalize: whether or not to normalize each image by the CIFAR dataset\n","        mean and stddev.\n","      try_gcs: Whether or not to try to use the GCS stored versions of dataset\n","        files.\n","      download_data: Whether or not to download data before loading.\n","      use_bfloat16: Whether or not to load the data in bfloat16 or float32.\n","      aug_params: hyperparameters for the data augmentation pre-processing.\n","      data_dir: Directory to read/write data, that is passed to the\n","        tfds dataset_builder as a data_dir parameter.\n","      is_training: Whether or not the given `split` is the training split. Only\n","        required when the passed split is not one of ['train', 'validation',\n","        'test', tfds.Split.TRAIN, tfds.Split.VALIDATION, tfds.Split.TEST].\n","    \"\"\"\n","    self._normalize = normalize\n","    dataset_builder = tfds.builder(\n","        name, \n","        #try_gcs=try_gcs,\n","        data_dir=data_dir)\n","    if is_training is None:\n","      is_training = split in ['train', tfds.Split.TRAIN]\n","    new_split = base.get_validation_percent_split(\n","        dataset_builder, validation_percent, split)\n","    super(_CifarDataset, self).__init__(\n","        name=name,\n","        dataset_builder=dataset_builder,\n","        split=new_split,\n","        seed=seed,\n","        is_training=is_training,\n","        shuffle_buffer_size=shuffle_buffer_size,\n","        num_parallel_parser_calls=num_parallel_parser_calls,\n","        drop_remainder=drop_remainder,\n","        fingerprint_key=fingerprint_key,\n","        download_data=download_data,\n","        cache=True)\n","\n","    self._use_bfloat16 = use_bfloat16\n","    if aug_params is None:\n","      aug_params = {}\n","    self._adaptive_mixup = aug_params.get('adaptive_mixup', False)\n","    ensemble_size = aug_params.get('ensemble_size', 1)\n","    if self._adaptive_mixup and 'mixup_coeff' not in aug_params:\n","      # Hard target in the first epoch!\n","      aug_params['mixup_coeff'] = tf.ones([ensemble_size, 10])\n","    self._aug_params = aug_params\n","\n","  def _create_process_example_fn(self) -> base.PreProcessFn:\n","\n","    def _example_parser(example: types.Features) -> types.Features:\n","      \"\"\"A pre-process function to return images in [0, 1].\"\"\"\n","      image = example['image']\n","      image_dtype = tf.bfloat16 if self._use_bfloat16 else tf.float32\n","      use_augmix = self._aug_params.get('augmix', False)\n","      if self._is_training:\n","        image_shape = tf.shape(image)\n","        # Expand the image by 2 pixels, then crop back down to 32x32.\n","        image = tf.image.resize_with_crop_or_pad(\n","            image, image_shape[0] + 4, image_shape[1] + 4)\n","        # Note that self._seed will already be shape (2,), as is required for\n","        # stateless random ops, and so will per_example_step_seed.\n","        per_example_step_seed = tf.random.experimental.stateless_fold_in(\n","            self._seed, example[self._enumerate_id_key])\n","        # per_example_step_seeds will be of size (num, 3).\n","        # First for random_crop, second for flip, third optionally for\n","        # RandAugment, and foruth optionally for Augmix.\n","        per_example_step_seeds = tf.random.experimental.stateless_split(\n","            per_example_step_seed, num=4)\n","        image = tf.image.stateless_random_crop(\n","            image,\n","            (image_shape[0], image_shape[0], 3),\n","            seed=per_example_step_seeds[0])\n","        image = tf.image.stateless_random_flip_left_right(\n","            image,\n","            seed=per_example_step_seeds[1])\n","\n","        # Only random augment for now.\n","        if self._aug_params.get('random_augment', False):\n","          count = self._aug_params['aug_count']\n","          augment_seeds = tf.random.experimental.stateless_split(\n","              per_example_step_seeds[2], num=count)\n","          augmenter = augment_utils.RandAugment()\n","          augmented = [\n","              augmenter.distort(image, seed=augment_seeds[c])\n","              for c in range(count)\n","          ]\n","          image = tf.stack(augmented)\n","\n","        if use_augmix:\n","          augmenter = augment_utils.RandAugment()\n","          image = augmix.do_augmix(\n","              image, self._aug_params, augmenter, image_dtype,\n","              mean=CIFAR10_MEAN, std=CIFAR10_STD,\n","              seed=per_example_step_seeds[3])\n","\n","      # The image has values in the range [0, 1].\n","      # Optionally normalize by the dataset statistics.\n","      if not use_augmix:\n","        if self._normalize:\n","          image = augmix.normalize_convert_image(\n","              image, image_dtype, mean=CIFAR10_MEAN, std=CIFAR10_STD)\n","        else:\n","          image = tf.image.convert_image_dtype(image, image_dtype)\n","      parsed_example = example.copy()\n","      parsed_example['features'] = image\n","\n","      # Note that labels are always float32, even when images are bfloat16.\n","      mixup_alpha = self._aug_params.get('mixup_alpha', 0)\n","      label_smoothing = self._aug_params.get('label_smoothing', 0.)\n","      should_onehot = mixup_alpha > 0 or label_smoothing > 0\n","      if should_onehot:\n","        parsed_example['labels'] = tf.one_hot(\n","            example['label'], 10, dtype=tf.float32)\n","      else:\n","        parsed_example['labels'] = tf.cast(example['label'], tf.float32)\n","\n","      del parsed_example['image']\n","      del parsed_example['label']\n","      return parsed_example\n","\n","    return _example_parser\n","\n","  def _create_process_batch_fn(\n","      self,\n","      batch_size: int) -> Optional[base.PreProcessFn]:\n","    if self._is_training and self._aug_params.get('mixup_alpha', 0) > 0:\n","      if self._adaptive_mixup:\n","        return _tuple_dict_fn_converter(\n","            augmix.adaptive_mixup, batch_size, self._aug_params)\n","      else:\n","        return _tuple_dict_fn_converter(\n","            augmix.mixup, batch_size, self._aug_params)\n","    return None\n","\n","\n","class Cifar10Dataset(_CifarDataset):\n","  \"\"\"CIFAR10 dataset builder class.\"\"\"\n","\n","  def __init__(self, **kwargs):\n","    super(Cifar10Dataset, self).__init__(\n","        name='cifar10',\n","        fingerprint_key='id',\n","        **kwargs)\n","\n","\n","class Cifar100Dataset(_CifarDataset):\n","  \"\"\"CIFAR100 dataset builder class.\"\"\"\n","\n","  def __init__(self, **kwargs):\n","    super(Cifar100Dataset, self).__init__(\n","        name='cifar100',\n","        fingerprint_key='id',\n","        **kwargs)\n","\n","\n","class Cifar10CorruptedDataset(_CifarDataset):\n","  \"\"\"CIFAR10-C dataset builder class.\"\"\"\n","\n","  def __init__(\n","      self,\n","      corruption_type: str,\n","      severity: int,\n","      **kwargs):\n","    \"\"\"Create a CIFAR10-C tf.data.Dataset builder.\n","    Args:\n","      corruption_type: Corruption name.\n","      severity: Corruption severity, an integer between 1 and 5.\n","      **kwargs: Additional keyword arguments.\n","    \"\"\"\n","    super(Cifar10CorruptedDataset, self).__init__(\n","        name=f'cifar10_corrupted/{corruption_type}_{severity}',\n","        fingerprint_key=None,\n","        **kwargs)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/edward2/__init__.py:34: UserWarning: JAX backend for Edward2 is not available.\n","  warnings.warn(\"JAX backend for Edward2 is not available.\")\n","WARNING:absl:No module named 'flax'\n","WARNING:absl:No module named 'tensorflow_addons'\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"bKR3NGUEcyQm"},"source":["# datasets"]},{"cell_type":"code","metadata":{"id":"iV-aaJyuacx1","executionInfo":{"status":"ok","timestamp":1623391644784,"user_tz":-540,"elapsed":7,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["#datasets\n","\n","# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","# Lint as: python3\n","\"\"\"Dataset getter utility.\"\"\"\n","\n","import json\n","import logging\n","from typing import Any, List, Tuple, Union\n","import warnings\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from uncertainty_baselines.datasets.base import BaseDataset\n","from uncertainty_baselines.datasets.cifar import Cifar100Dataset\n","from uncertainty_baselines.datasets.cifar import Cifar10CorruptedDataset\n","#from uncertainty_baselines.datasets.cifar import Cifar10Dataset\n","from uncertainty_baselines.datasets.cifar100_corrupted import Cifar100CorruptedDataset\n","from uncertainty_baselines.datasets.clinc_intent import ClincIntentDetectionDataset\n","from uncertainty_baselines.datasets.criteo import CriteoDataset\n","from uncertainty_baselines.datasets.diabetic_retinopathy_detection import DiabeticRetinopathyDetectionDataset\n","from uncertainty_baselines.datasets.genomics_ood import GenomicsOodDataset\n","from uncertainty_baselines.datasets.glue import GlueDatasets\n","from uncertainty_baselines.datasets.imagenet import ImageNetDataset\n","from uncertainty_baselines.datasets.mnist import MnistDataset\n","from uncertainty_baselines.datasets.mnli import MnliDataset\n","from uncertainty_baselines.datasets.movielens import MovieLensDataset\n","from uncertainty_baselines.datasets.places import Places365Dataset\n","from uncertainty_baselines.datasets.random import RandomGaussianImageDataset\n","from uncertainty_baselines.datasets.random import RandomRademacherImageDataset\n","from uncertainty_baselines.datasets.svhn import SvhnDataset\n","from uncertainty_baselines.datasets.toxic_comments import CivilCommentsDataset\n","from uncertainty_baselines.datasets.toxic_comments import CivilCommentsIdentitiesDataset\n","from uncertainty_baselines.datasets.toxic_comments import WikipediaToxicityDataset\n","\n","try:\n","  from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n","except ImportError as e:\n","  warnings.warn(f'Skipped due to ImportError: {e}')\n","  SpeechCommandsDataset = None\n","\n","DATASETS = {\n","    'cifar100': Cifar100Dataset,\n","    'cifar10': Cifar10Dataset,\n","    'cifar10_corrupted': Cifar10CorruptedDataset,\n","    'cifar100_corrupted': Cifar100CorruptedDataset,\n","    'civil_comments': CivilCommentsDataset,\n","    'civil_comments_identities': CivilCommentsIdentitiesDataset,\n","    'clinic_intent': ClincIntentDetectionDataset,\n","    'criteo': CriteoDataset,\n","    'diabetic_retinopathy_detection': DiabeticRetinopathyDetectionDataset,\n","    'imagenet': ImageNetDataset,\n","    'mnist': MnistDataset,\n","    'mnli': MnliDataset,\n","    'movielens': MovieLensDataset,\n","    'places365': Places365Dataset,\n","    'random_gaussian': RandomGaussianImageDataset,\n","    'random_rademacher': RandomRademacherImageDataset,\n","    'speech_commands': SpeechCommandsDataset,\n","    'svhn_cropped': SvhnDataset,\n","    'glue/cola': GlueDatasets['glue/cola'],\n","    'glue/sst2': GlueDatasets['glue/sst2'],\n","    'glue/mrpc': GlueDatasets['glue/mrpc'],\n","    'glue/qqp': GlueDatasets['glue/qqp'],\n","    'glue/qnli': GlueDatasets['glue/qnli'],\n","    'glue/rte': GlueDatasets['glue/rte'],\n","    'glue/wnli': GlueDatasets['glue/wnli'],\n","    'glue/stsb': GlueDatasets['glue/stsb'],\n","    'wikipedia_toxicity': WikipediaToxicityDataset,\n","    'genomics_ood': GenomicsOodDataset,\n","}\n","\n","\n","def get_dataset_names() -> List[str]:\n","  return list(DATASETS.keys())\n","\n","\n","def get(\n","    dataset_name: str,\n","    split: Union[Tuple[str, float], str, tfds.Split],\n","    **hyperparameters: Any) -> BaseDataset:\n","  \"\"\"Gets a dataset builder by name.\n","  Note that the user still needs to call\n","  `distribution_strategy.experimental_distribute_dataset(dataset)` on the loaded\n","  dataset if they are running in a distributed environment.\n","  Args:\n","    dataset_name: Name of the dataset builder class.\n","    split: a dataset split, either a custom tfds.Split or one of the\n","      tfds.Split enums [TRAIN, VALIDAITON, TEST] or their lowercase string\n","      names.\n","    **hyperparameters: dict of possible kwargs to be passed to the dataset\n","      constructor.\n","  Returns:\n","    A dataset builder class with a method .build(split) which can be called to\n","    get the tf.data.Dataset, which has elements that are a dict described by\n","    dataset_builder.info.\n","  Raises:\n","    ValueError: If dataset_name is unrecognized.\n","  \"\"\"\n","  hyperparameters_py = {\n","      k: (v.numpy().tolist() if isinstance(v, tf.Tensor) else v)\n","      for k, v in hyperparameters.items()\n","  }\n","  logging.info(\n","      'Building dataset %s with additional kwargs:\\n%s',\n","      dataset_name,\n","      json.dumps(hyperparameters_py, indent=2, sort_keys=True))\n","  if dataset_name not in DATASETS:\n","    raise ValueError('Unrecognized dataset name: {!r}'.format(dataset_name))\n","\n","  dataset_class = DATASETS[dataset_name]\n","  return dataset_class(\n","      split=split,\n","      **hyperparameters)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358,"referenced_widgets":["b444023624bd44c4b08e9f780c6b1473","43d949843c734f6e9d2ef24726667c26","c7205b957e724c3ebdb4a161622879f2","a6ad4050a4a340b893b87f4d77f00e2c","5a7d54ea535440cba02070a23d4b9a43","4cc3959c07a340cdacd528b68db8572d","27128ec958e24271a8fe0e01235c222b","36126a58f19c4baf8bbf9fe6beb47465","7b2424d926114aae99917aeee01405dc","4eaa57c3ff494b34949d70ec4becdcf9","6b2d6c76730f43e0b727f4d13f7ba473","cacc21054f3a40f28df5ad2663615798","6bdf10bd24894457bcd4483afc8aeedc","8d5f64c5b5ed4fd7aae5c5611b6abab5","60cb783686884e70a893c8e71853445f","9f0644d5cbc14328914e1aed3e82da19","1fe5b02a125542e5b20bd45fc8251fea","d7190eeb37a04346893a500e67b544bd","f51eeb4751eb4709b22ba7e089329755","18e6497287964684a0964d6249d8504a","d0876294fc2548e3bf8fc48d16eeeb47","159b7175271945f1ae5ecec70830f378","241c5816527f427690769335f894c244","8ffc1c827cd7415f8bfa063734388580","2b2ed2895b8d414f99dbd935ee05789b","7730ea5a80c54d1d88e4a704c9aaea0c","42c1b5191b524384a5cde2a12652bf17","87f09001b8a54666a0e12adc01d5612e","af345b9e265f43aa8e1c561e8fa2bcf4","5d218185109e446ca1fc6c4b5030b911","39fbbbcaff044d52b83d0363529ed70b","3ab5b07e5eda4d99968663f250972b3f","e6a6b6aa1e17498fbb4e6c227f2c1e60","32a78bbb459d45df9e68db657d1f1e8a","832e069a842e46dcb818a077c4c13cdf","96508d49248f4282b27c9dd53c946075","c811c7a653de4d049170ad09b3ae7745","219eb0c2ea04426284fecd5828718d92","b0b6c96fc37d4664bc19c23cc58e8d76","19de116dd89c4c1ab8ffc7bc6adf6ee8","b8c269ceb8ad44ee9a0af007226ad376","99e9d94bd69d4428b19acac9cc473550","6d064b0b31ec45acbac0c111225345ba","7e43d76cd25244338ffbae5edae72353","4a7ec57a8c564af1851c76a99c2d2228","789ab1c3f09a48f9930f03ccd728d45b","ad0aeda093dd4871aca71c3f2918e3a2","ae92f2d5706e45389cd8faf43af500f4","492b6fb9f1dd49338cfc853d0fa60895","742a0ad37b304f25ba38d3854f05501a","5188eb145a104021aafad581bdb1f690","e26ee93000ae4de88b2893b0c96cc560","f27371e2c9e34479997c317deafef629","38abbfd97ac842bbb0ea2e3adc47c13b","0a8ddb633aad4e678edd3d04d1f9e56d","9b0356a2b86d4e1a8527f6f756611d0b"]},"id":"_rWVYr4WgNAU","executionInfo":{"status":"ok","timestamp":1623391693810,"user_tz":-540,"elapsed":49032,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"079c07ae-524a-4bc8-c000-36d417b07dd6"},"source":["dataset_builder = tfds.builder('cifar10')\n","dataset_builder.download_and_prepare()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset cifar10/3.0.2 (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /root/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b444023624bd44c4b08e9f780c6b1473","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b2424d926114aae99917aeee01405dc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fe5b02a125542e5b20bd45fc8251fea","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b2ed2895b8d414f99dbd935ee05789b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/cifar10/3.0.2.incompleteFAN70C/cifar10-train.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6a6b6aa1e17498fbb4e6c227f2c1e60","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8c269ceb8ad44ee9a0af007226ad376","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/cifar10/3.0.2.incompleteFAN70C/cifar10-test.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"492b6fb9f1dd49338cfc853d0fa60895","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[1mDataset cifar10 downloaded and prepared to /root/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l20R8Qegh9a-"},"source":["# 모델"]},{"cell_type":"code","metadata":{"id":"uLoqWHV1h6BZ","executionInfo":{"status":"ok","timestamp":1623391694268,"user_tz":-540,"elapsed":476,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","# Lint as: python3\n","\"\"\"Wide Residual Network.\"\"\"\n","\n","import functools\n","from typing import Any, Dict, Iterable, Optional\n","\n","import tensorflow as tf\n","\n","HP_KEYS = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","\n","BatchNormalization = functools.partial(  # pylint: disable=invalid-name\n","    tf.keras.layers.BatchNormalization,\n","    epsilon=1e-5,  # using epsilon and momentum defaults from Torch\n","    momentum=0.9)\n","\n","\n","def Conv2D(filters, seed=None, **kwargs):  # pylint: disable=invalid-name\n","  \"\"\"Conv2D layer that is deterministically initialized.\"\"\"\n","  default_kwargs = {\n","      'kernel_size': 3,\n","      'padding': 'same',\n","      'use_bias': False,\n","      # Note that we need to use the class constructor for the initializer to\n","      # get deterministic initialization.\n","      'kernel_initializer': tf.keras.initializers.HeNormal(seed=seed),\n","  }\n","  # Override defaults with the passed kwargs.\n","  default_kwargs.update(kwargs)\n","  return tf.keras.layers.Conv2D(filters, **default_kwargs)\n","\n","\n","def basic_block(\n","    inputs: tf.Tensor,\n","    filters: int,\n","    strides: int,\n","    conv_l2: float,\n","    bn_l2: float,\n","    seed: int,\n","    version: int) -> tf.Tensor:\n","  \"\"\"Basic residual block of two 3x3 convs.\n","  Args:\n","    inputs: tf.Tensor.\n","    filters: Number of filters for Conv2D.\n","    strides: Stride dimensions for Conv2D.\n","    conv_l2: L2 regularization coefficient for the conv kernels.\n","    bn_l2: L2 regularization coefficient for the batch norm layers.\n","    seed: random seed used for initialization.\n","    version: 1, indicating the original ordering from He et al. (2015); or 2,\n","      indicating the preactivation ordering from He et al. (2016).\n","  Returns:\n","    tf.Tensor.\n","  \"\"\"\n","  x = inputs\n","  y = inputs\n","  if version == 2:\n","    y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(bn_l2),\n","                           gamma_regularizer=tf.keras.regularizers.l2(bn_l2))(y)\n","    y = tf.keras.layers.Activation('relu')(y)\n","  seeds = tf.random.experimental.stateless_split([seed, seed + 1], 3)[:, 0]\n","  y = Conv2D(filters,\n","             strides=strides,\n","             seed=seeds[0],\n","             kernel_regularizer=tf.keras.regularizers.l2(conv_l2))(y)\n","  y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(bn_l2),\n","                         gamma_regularizer=tf.keras.regularizers.l2(bn_l2))(y)\n","  y = tf.keras.layers.Activation('relu')(y)\n","  y = Conv2D(filters,\n","             strides=1,\n","             seed=seeds[1],\n","             kernel_regularizer=tf.keras.regularizers.l2(conv_l2))(y)\n","  if version == 1:\n","    y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(bn_l2),\n","                           gamma_regularizer=tf.keras.regularizers.l2(bn_l2))(y)\n","  if not x.shape.is_compatible_with(y.shape):\n","    x = Conv2D(filters,\n","               kernel_size=1,\n","               strides=strides,\n","               seed=seeds[2],\n","               kernel_regularizer=tf.keras.regularizers.l2(conv_l2))(x)\n","  x = tf.keras.layers.add([x, y])\n","  if version == 1:\n","    x = tf.keras.layers.Activation('relu')(x)\n","  return x\n","\n","\n","def group(inputs, filters, strides, num_blocks, conv_l2, bn_l2, version, seed):\n","  \"\"\"Group of residual blocks.\"\"\"\n","  seeds = tf.random.experimental.stateless_split(\n","      [seed, seed + 1], num_blocks)[:, 0]\n","  x = basic_block(\n","      inputs,\n","      filters=filters,\n","      strides=strides,\n","      conv_l2=conv_l2,\n","      bn_l2=bn_l2,\n","      version=version,\n","      seed=seeds[0])\n","  for i in range(num_blocks - 1):\n","    x = basic_block(\n","        x,\n","        filters=filters,\n","        strides=1,\n","        conv_l2=conv_l2,\n","        bn_l2=bn_l2,\n","        version=version,\n","        seed=seeds[i + 1])\n","  return x\n","\n","\n","def _parse_hyperparameters(l2: float, hps: Dict[str, float]):\n","  \"\"\"Extract the L2 parameters for the dense, conv and batch-norm layers.\"\"\"\n","\n","  assert_msg = ('Ambiguous hyperparameter specifications: either l2 or hps '\n","                'must be provided (received {} and {}).'.format(l2, hps))\n","  is_specified = lambda h: bool(h) and all(v is not None for v in h.values())\n","  only_l2_is_specified = l2 is not None and not is_specified(hps)\n","  only_hps_is_specified = l2 is None and is_specified(hps)\n","  assert only_l2_is_specified or only_hps_is_specified, assert_msg\n","  if only_hps_is_specified:\n","    assert_msg = 'hps must contain the keys {}!={}.'.format(HP_KEYS, hps.keys())\n","    assert set(hps.keys()).issuperset(HP_KEYS), assert_msg\n","    return hps\n","  else:\n","    return {k: l2 for k in HP_KEYS}\n","\n","\n","def wide_resnet(\n","    input_shape: Iterable[int],\n","    depth: int,\n","    width_multiplier: int,\n","    num_classes: int,\n","    l2: float,\n","    version: int = 2,\n","    seed: int = 42,\n","    hps: Optional[Dict[str, float]] = None) -> tf.keras.models.Model:\n","  \"\"\"Builds Wide ResNet.\n","  Following Zagoruyko and Komodakis (2016), it accepts a width multiplier on the\n","  number of filters. Using three groups of residual blocks, the network maps\n","  spatial features of size 32x32 -> 16x16 -> 8x8.\n","  Args:\n","    input_shape: tf.Tensor.\n","    depth: Total number of convolutional layers. \"n\" in WRN-n-k. It differs from\n","      He et al. (2015)'s notation which uses the maximum depth of the network\n","      counting non-conv layers like dense.\n","    width_multiplier: Integer to multiply the number of typical filters by. \"k\"\n","      in WRN-n-k.\n","    num_classes: Number of output classes.\n","    l2: L2 regularization coefficient.\n","    version: 1, indicating the original ordering from He et al. (2015); or 2,\n","      indicating the preactivation ordering from He et al. (2016).\n","    seed: random seed used for initialization.\n","    hps: Fine-grained specs of the hyperparameters, as a Dict[str, float].\n","  Returns:\n","    tf.keras.Model.\n","  \"\"\"\n","  l2_reg = tf.keras.regularizers.l2\n","  hps = _parse_hyperparameters(l2, hps)\n","\n","  seeds = tf.random.experimental.stateless_split([seed, seed + 1], 5)[:, 0]\n","  if (depth - 4) % 6 != 0:\n","    raise ValueError('depth should be 6n+4 (e.g., 16, 22, 28, 40).')\n","  num_blocks = (depth - 4) // 6\n","  inputs = tf.keras.layers.Input(shape=input_shape)\n","  x = Conv2D(16,\n","             strides=1,\n","             seed=seeds[0],\n","             kernel_regularizer=l2_reg(hps['input_conv_l2']))(inputs)\n","  if version == 1:\n","    x = BatchNormalization(beta_regularizer=l2_reg(hps['bn_l2']),\n","                           gamma_regularizer=l2_reg(hps['bn_l2']))(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","  x = group(x,\n","            filters=16 * width_multiplier,\n","            strides=1,\n","            num_blocks=num_blocks,\n","            conv_l2=hps['group_1_conv_l2'],\n","            bn_l2=hps['bn_l2'],\n","            version=version,\n","            seed=seeds[1])\n","  x = group(x,\n","            filters=32 * width_multiplier,\n","            strides=2,\n","            num_blocks=num_blocks,\n","            conv_l2=hps['group_2_conv_l2'],\n","            bn_l2=hps['bn_l2'],\n","            version=version,\n","            seed=seeds[2])\n","  x = group(x,\n","            filters=64 * width_multiplier,\n","            strides=2,\n","            num_blocks=num_blocks,\n","            conv_l2=hps['group_3_conv_l2'],\n","            bn_l2=hps['bn_l2'],\n","            version=version,\n","            seed=seeds[3])\n","  if version == 2:\n","    x = BatchNormalization(beta_regularizer=l2_reg(hps['bn_l2']),\n","                           gamma_regularizer=l2_reg(hps['bn_l2']))(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n","  x = tf.keras.layers.Flatten()(x)\n","  x = tf.keras.layers.Dense(\n","      num_classes,\n","      kernel_initializer=tf.keras.initializers.HeNormal(seed=seeds[4]),\n","      kernel_regularizer=l2_reg(hps['dense_kernel_l2']),\n","      bias_regularizer=l2_reg(hps['dense_bias_l2']))(x)\n","  return tf.keras.Model(\n","      inputs=inputs,\n","      outputs=x,\n","      name='wide_resnet-{}-{}'.format(depth, width_multiplier))\n","\n","\n","def create_model(\n","    batch_size: Optional[int],\n","    depth: int,\n","    width_multiplier: int,\n","    input_shape: Iterable[int] = (32, 32, 3),\n","    num_classes: int = 10,\n","    l2_weight: float = 0.0,\n","    version: int = 2,\n","    **unused_kwargs: Dict[str, Any]) -> tf.keras.models.Model:\n","  \"\"\"Creates model.\"\"\"\n","  del batch_size  # unused arg\n","  return wide_resnet(input_shape=input_shape,\n","                     depth=depth,\n","                     width_multiplier=width_multiplier,\n","                     num_classes=num_classes,\n","                     l2=l2_weight,\n","                     version=version)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gVUYU9qlcOw","executionInfo":{"status":"ok","timestamp":1623391694269,"user_tz":-540,"elapsed":3,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["# try:\n","#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","#     print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","# except ValueError:\n","#     tpu = None\n","\n","# if tpu:\n","#     tf.config.experimental_connect_to_cluster(tpu)\n","#     tf.tpu.experimental.initialize_tpu_system(tpu)\n","#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","# else:\n","#     strategy = tf.distribute.get_strategy()\n","\n","# print(\"REPLICAS: \", strategy.num_replicas_in_sync)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZB84Bamhc2q_"},"source":["# 중요코드"]},{"cell_type":"code","metadata":{"id":"sSEUvyFtSJWy","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1623391944060,"user_tz":-540,"elapsed":35092,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"87131c62-a693-4774-db30-070ee90864de"},"source":["# coding=utf-8\n","# Copyright 2021 The Uncertainty Baselines Authors.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","\"\"\"Wide ResNet 28-10 on CIFAR-10/100 trained with maximum likelihood.\n","Hyperparameters differ slightly from the original paper's code\n","(https://github.com/szagoruyko/wide-residual-networks) as TensorFlow uses, for\n","example, l2 instead of weight decay, and a different parameterization for SGD's\n","momentum.\n","\"\"\"\n","\n","import os\n","import time\n","from absl import app\n","from absl import flags\n","from absl import logging\n","import robustness_metrics as rm\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import uncertainty_baselines as ub\n","import utils  # local file import\n","from tensorboard.plugins.hparams import api as hp\n","\n","# flags.DEFINE_string(\"f\", \"\", \"kernel\")\n","# flags.DEFINE_float('label_smoothing', 0., 'Label smoothing parameter in [0,1].')\n","# flags.register_validator('label_smoothing',\n","#                          lambda ls: ls >= 0.0 and ls <= 1.0,\n","#                          message='--label_smoothing must be in [0, 1].')\n","\n","# # Data Augmentation flags.\n","# flags.DEFINE_bool('augmix', False,\n","#                   'Whether to perform AugMix [4] on the input data.')\n","# flags.DEFINE_integer('aug_count', 1,\n","#                      'Number of augmentation operations in AugMix to perform '\n","#                      'on the input image. In the simgle model context, it'\n","#                      'should be 1. In the ensembles context, it should be'\n","#                      'ensemble_size if we perform random_augment only; It'\n","#                      'should be (ensemble_size - 1) if we perform augmix.')\n","# flags.DEFINE_float('augmix_prob_coeff', 0.5, 'Augmix probability coefficient.')\n","# flags.DEFINE_integer('augmix_depth', -1,\n","#                      'Augmix depth, -1 meaning sampled depth. This corresponds'\n","#                      'to line 7 in the Algorithm box in [4].')\n","# flags.DEFINE_integer('augmix_width', 3,\n","#                      'Augmix width. This corresponds to the k in line 5 in the'\n","#                      'Algorithm box in [4].')\n","\n","# # Fine-grained specification of the hyperparameters (used when FLAGS.l2 is None)\n","# flags.DEFINE_float('bn_l2', None, 'L2 reg. coefficient for batch-norm layers.')\n","# flags.DEFINE_float('input_conv_l2', None,\n","#                    'L2 reg. coefficient for the input conv layer.')\n","# flags.DEFINE_float('group_1_conv_l2', None,\n","#                    'L2 reg. coefficient for the 1st group of conv layers.')\n","# flags.DEFINE_float('group_2_conv_l2', None,\n","#                    'L2 reg. coefficient for the 2nd group of conv layers.')\n","# flags.DEFINE_float('group_3_conv_l2', None,\n","#                    'L2 reg. coefficient for the 3rd group of conv layers.')\n","# flags.DEFINE_float('dense_kernel_l2', None,\n","#                    'L2 reg. coefficient for the kernel of the dense layer.')\n","# flags.DEFINE_float('dense_bias_l2', None,\n","#                    'L2 reg. coefficient for the bias of the dense layer.')\n","\n","\n","# flags.DEFINE_bool('collect_profile', False,\n","#                   'Whether to trace a profile with tensorboard')\n","\n","# FLAGS = flags.FLAGS\n","\n","\n","# def _extract_hyperparameter_dictionary():\n","#   \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","#   flags_as_dict = FLAGS.flag_values_dict()\n","#   hp_keys = ub.models.models.wide_resnet.HP_KEYS\n","#   hps = {k: flags_as_dict[k] for k in hp_keys}\n","#   return hps\n","\n","def _extract_hyperparameter_dictionary():\n","  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","  hp_keys = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","  hps = {'bn_l2':None, 'input_conv_l2':None, 'group_1_conv_l2':None, 'group_2_conv_l2':None,\n","           'group_3_conv_l2':None, 'dense_kernel_l2':None, 'dense_bias_l2':None}\n","  return hps  \n","\n","\n","def main(argv):\n","\n","\n","  try:\n","      tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","      print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","  except ValueError:\n","      tpu = None\n","\n","  if tpu:\n","      tf.config.experimental_connect_to_cluster(tpu)\n","      tf.tpu.experimental.initialize_tpu_system(tpu)\n","      strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","  else:\n","      strategy = tf.distribute.get_strategy()\n","\n","  print(\"REPLICAS: \", strategy.num_replicas_in_sync)  \n","\n","\n","\n","  fmt = '[%(filename)s:%(lineno)s] %(message)s'\n","  formatter = logging.PythonFormatter(fmt)\n","  logging.get_absl_handler().setFormatter(formatter)\n","  del argv  # unused arg\n","\n","  tf.io.gfile.makedirs(FLAGS.output_dir)\n","  logging.info('Saving checkpoints at %s', FLAGS.output_dir)\n","  tf.random.set_seed(FLAGS.seed)\n","\n","  data_dir = utils.get_data_dir_from_flags(FLAGS)\n","  if FLAGS.use_gpu:\n","    logging.info('Use GPU')\n","    strategy = tf.distribute.MirroredStrategy()\n","  else:\n","    logging.info('Use TPU at %s',\n","                 FLAGS.tpu if FLAGS.tpu is not None else 'local')\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.TPUStrategy(resolver)\n","\n","  ds_info = tfds.builder(FLAGS.dataset).info\n","  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n","  train_dataset_size = (\n","      ds_info.splits['train'].num_examples * FLAGS.train_proportion)\n","  steps_per_epoch = int(train_dataset_size / batch_size)\n","  logging.info('Steps per epoch %s', steps_per_epoch)\n","  logging.info('Size of the dataset %s', ds_info.splits['train'].num_examples)\n","  logging.info('Train proportion %s', FLAGS.train_proportion)\n","  steps_per_eval = ds_info.splits['test'].num_examples // batch_size\n","  num_classes = ds_info.features['label'].num_classes\n","\n","  aug_params = {\n","      'augmix': FLAGS.augmix,\n","      'aug_count': FLAGS.aug_count,\n","      'augmix_depth': FLAGS.augmix_depth,\n","      'augmix_prob_coeff': FLAGS.augmix_prob_coeff,\n","      'augmix_width': FLAGS.augmix_width,\n","  }\n","\n","  # Note that stateless_{fold_in,split} may incur a performance cost, but a\n","  # quick side-by-side test seemed to imply this was minimal.\n","\n","  seeds = tf.random.experimental.stateless_split(\n","      [FLAGS.seed, FLAGS.seed + 1], 2)[:, 0]\n","  train_builder = get(\n","      FLAGS.dataset,\n","      data_dir=data_dir,\n","      download_data=FLAGS.download_data,\n","      split=tfds.Split.TRAIN,\n","      seed=seeds[0],\n","      aug_params=aug_params,\n","      validation_percent=1. - FLAGS.train_proportion,)\n","  train_dataset = train_builder.load(batch_size=batch_size)\n","  validation_dataset = None\n","  steps_per_validation = 0\n","  if FLAGS.train_proportion < 1.0:\n","    validation_builder = get(\n","        FLAGS.dataset,\n","        split=tfds.Split.VALIDATION,\n","        validation_percent=1. - FLAGS.train_proportion,\n","        data_dir=data_dir)\n","    validation_dataset = validation_builder.load(batch_size=batch_size)\n","    validation_dataset = strategy.experimental_distribute_dataset(\n","        validation_dataset)\n","    steps_per_validation = validation_builder.num_examples // batch_size\n","  clean_test_builder = get(\n","      FLAGS.dataset,\n","      split=tfds.Split.TEST,\n","      data_dir=data_dir)\n","  clean_test_dataset = clean_test_builder.load(batch_size=batch_size)\n","  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","  test_datasets = {\n","      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n","  }\n","  steps_per_epoch = train_builder.num_examples // batch_size\n","  steps_per_eval = clean_test_builder.num_examples // batch_size\n","  num_classes = 100 if FLAGS.dataset == 'cifar100' else 10\n","  if FLAGS.corruptions_interval > 0:\n","    if FLAGS.dataset == 'cifar100':\n","      data_dir = FLAGS.cifar100_c_path\n","    corruption_types, _ = utils.load_corrupted_test_info(FLAGS.dataset)\n","    for corruption_type in corruption_types:\n","      for severity in range(1, 6):\n","        dataset = get(\n","            f'{FLAGS.dataset}_corrupted',\n","            corruption_type=corruption_type,\n","            severity=severity,\n","            split=tfds.Split.TEST,\n","            data_dir=data_dir).load(batch_size=batch_size)\n","        test_datasets[f'{corruption_type}_{severity}'] = (\n","            strategy.experimental_distribute_dataset(dataset))\n","\n","  summary_writer = tf.summary.create_file_writer(\n","      os.path.join(FLAGS.output_dir, 'summaries'))\n","\n","  with strategy.scope():\n","    logging.info('Building ResNet model')\n","\n","\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","    except ValueError:\n","        tpu = None\n","\n","    if tpu:\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        tf.tpu.experimental.initialize_tpu_system(tpu)\n","        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","    else:\n","        strategy = tf.distribute.get_strategy()\n","\n","    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","\n","\n","    model = wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=FLAGS.l2,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=seeds[1])\n","    logging.info('Model input shape: %s', model.input_shape)\n","    logging.info('Model output shape: %s', model.output_shape)\n","    logging.info('Model number of weights: %s', model.count_params())\n","    # Linearly scale learning rate and the decay epochs by vanilla settings.\n","    base_lr = FLAGS.base_learning_rate * batch_size / 128\n","    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n","                       for start_epoch_str in FLAGS.lr_decay_epochs]\n","    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule(\n","        steps_per_epoch,\n","        base_lr,\n","        decay_ratio=FLAGS.lr_decay_ratio,\n","        decay_epochs=lr_decay_epochs,\n","        warmup_epochs=FLAGS.lr_warmup_epochs)\n","    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n","                                        momentum=1.0 - FLAGS.one_minus_momentum,\n","                                        nesterov=True)\n","    metrics = {\n","        'train/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'train/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'train/loss':\n","            tf.keras.metrics.Mean(),\n","        'train/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n","        'test/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'test/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'test/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n","    }\n","    if validation_dataset:\n","      metrics.update({\n","          'validation/negative_log_likelihood': tf.keras.metrics.Mean(),\n","          'validation/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n","          'validation/ece': rm.metrics.ExpectedCalibrationError(\n","              num_bins=FLAGS.num_bins),\n","      })\n","    if FLAGS.corruptions_interval > 0:\n","      corrupt_metrics = {}\n","      for intensity in range(1, 6):\n","        for corruption in corruption_types:\n","          dataset_name = '{0}_{1}'.format(corruption, intensity)\n","          corrupt_metrics['test/nll_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.Mean())\n","          corrupt_metrics['test/accuracy_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.SparseCategoricalAccuracy())\n","          corrupt_metrics['test/ece_{}'.format(dataset_name)] = (\n","              rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n","\n","    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n","    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n","    initial_epoch = 0\n","    if latest_checkpoint:\n","      # checkpoint.restore must be within a strategy.scope() so that optimizer\n","      # slot variables are mirrored.\n","      checkpoint.restore(latest_checkpoint)\n","      logging.info('Loaded checkpoint %s', latest_checkpoint)\n","      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n","\n","  @tf.function\n","  def train_step(iterator):\n","    \"\"\"Training StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","\n","      if FLAGS.augmix and FLAGS.aug_count >= 1:\n","        # Index 0 at augmix processing is the unperturbed image.\n","        # We take just 1 augmented image from the returned augmented images.\n","        images = images[:, 1, ...]\n","      with tf.GradientTape() as tape:\n","        logits = model(images, training=True)\n","        if FLAGS.label_smoothing == 0.:\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.sparse_categorical_crossentropy(labels,\n","                                                              logits,\n","                                                              from_logits=True))\n","        else:\n","          one_hot_labels = tf.one_hot(tf.cast(labels, tf.int32), num_classes)\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.categorical_crossentropy(\n","                  one_hot_labels,\n","                  logits,\n","                  from_logits=True,\n","                  label_smoothing=FLAGS.label_smoothing))\n","        l2_loss = sum(model.losses)\n","        loss = negative_log_likelihood + l2_loss\n","        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n","        scaled_loss = loss / strategy.num_replicas_in_sync\n","\n","      grads = tape.gradient(scaled_loss, model.trainable_variables)\n","      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","      probs = tf.nn.softmax(logits)\n","      metrics['train/ece'].add_batch(probs, label=labels)\n","      metrics['train/loss'].update_state(loss)\n","      metrics['train/negative_log_likelihood'].update_state(\n","          negative_log_likelihood)\n","      metrics['train/accuracy'].update_state(labels, logits)\n","\n","    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  @tf.function\n","  def test_step(iterator, dataset_split, dataset_name, num_steps):\n","    \"\"\"Evaluation StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","      logits = model(images, training=False)\n","      probs = tf.nn.softmax(logits)\n","      negative_log_likelihood = tf.reduce_mean(\n","          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n","\n","      if dataset_name == 'clean':\n","        metrics[f'{dataset_split}/negative_log_likelihood'].update_state(\n","            negative_log_likelihood)\n","        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs)\n","        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels)\n","      else:\n","        corrupt_metrics['test/nll_{}'.format(dataset_name)].update_state(\n","            negative_log_likelihood)\n","        corrupt_metrics['test/accuracy_{}'.format(dataset_name)].update_state(\n","            labels, probs)\n","        corrupt_metrics['test/ece_{}'.format(dataset_name)].add_batch(\n","            probs, label=labels)\n","\n","    for _ in tf.range(tf.cast(num_steps, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n","  metrics.update({'train/ms_per_example': tf.keras.metrics.Mean()})\n","\n","  train_iterator = iter(train_dataset)\n","  start_time = time.time()\n","  tb_callback = None\n","  if FLAGS.collect_profile:\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        profile_batch=(100, 102),\n","        log_dir=os.path.join(FLAGS.output_dir, 'logs'))\n","    tb_callback.set_model(model)\n","  for epoch in range(initial_epoch, FLAGS.train_epochs):\n","    logging.info('Starting to run epoch: %s', epoch)\n","    if tb_callback:\n","      tb_callback.on_epoch_begin(epoch)\n","    train_start_time = time.time()\n","    train_step(train_iterator)\n","    ms_per_example = (time.time() - train_start_time) * 1e6 / batch_size\n","    metrics['train/ms_per_example'].update_state(ms_per_example)\n","\n","    current_step = (epoch + 1) * steps_per_epoch\n","    max_steps = steps_per_epoch * FLAGS.train_epochs\n","    time_elapsed = time.time() - start_time\n","    steps_per_sec = float(current_step) / time_elapsed\n","    eta_seconds = (max_steps - current_step) / steps_per_sec\n","    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n","               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n","                   current_step / max_steps,\n","                   epoch + 1,\n","                   FLAGS.train_epochs,\n","                   steps_per_sec,\n","                   eta_seconds / 60,\n","                   time_elapsed / 60))\n","    logging.info(message)\n","    if tb_callback:\n","      tb_callback.on_epoch_end(epoch)\n","\n","    if validation_dataset:\n","      validation_iterator = iter(validation_dataset)\n","      test_step(\n","          validation_iterator, 'validation', 'clean', steps_per_validation)\n","    datasets_to_evaluate = {'clean': test_datasets['clean']}\n","    if (FLAGS.corruptions_interval > 0 and\n","        (epoch + 1) % FLAGS.corruptions_interval == 0):\n","      datasets_to_evaluate = test_datasets\n","    for dataset_name, test_dataset in datasets_to_evaluate.items():\n","      test_iterator = iter(test_dataset)\n","      logging.info('Testing on dataset %s', dataset_name)\n","      logging.info('Starting to run eval at epoch: %s', epoch)\n","      test_start_time = time.time()\n","      test_step(test_iterator, 'test', dataset_name, steps_per_eval)\n","      ms_per_example = (time.time() - test_start_time) * 1e6 / batch_size\n","      metrics['test/ms_per_example'].update_state(ms_per_example)\n","\n","      logging.info('Done with testing on %s', dataset_name)\n","\n","    corrupt_results = {}\n","    if (FLAGS.corruptions_interval > 0 and\n","        (epoch + 1) % FLAGS.corruptions_interval == 0):\n","      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n","                                                        corruption_types)\n","\n","    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n","                 metrics['train/loss'].result(),\n","                 metrics['train/accuracy'].result() * 100)\n","    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n","                 metrics['test/negative_log_likelihood'].result(),\n","                 metrics['test/accuracy'].result() * 100)\n","    total_results = {name: metric.result() for name, metric in metrics.items()}\n","    total_results.update(corrupt_results)\n","    # Metrics from Robustness Metrics (like ECE) will return a dict with a\n","    # single key/value, instead of a scalar.\n","    total_results = {\n","        k: (list(v.values())[0] if isinstance(v, dict) else v)\n","        for k, v in total_results.items()\n","    }\n","    with summary_writer.as_default():\n","      for name, result in total_results.items():\n","        tf.summary.scalar(name, result, step=epoch + 1)\n","\n","    for metric in metrics.values():\n","      metric.reset_states()\n","\n","    if (FLAGS.checkpoint_interval > 0 and\n","        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n","      checkpoint_name = checkpoint.save(\n","          os.path.join(FLAGS.output_dir, 'checkpoint'))\n","      logging.info('Saved checkpoint to %s', checkpoint_name)\n","\n","  final_checkpoint_name = checkpoint.save(\n","      os.path.join(FLAGS.output_dir, 'checkpoint'))\n","  logging.info('Saved last checkpoint to %s', final_checkpoint_name)\n","  with summary_writer.as_default():\n","    hp.hparams({\n","        'base_learning_rate': FLAGS.base_learning_rate,\n","        'one_minus_momentum': FLAGS.one_minus_momentum,\n","        'l2': FLAGS.l2,\n","    })\n","\n","\n","if __name__ == '__main__':\n","  app.run(main)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Running on TPU  ['10.116.77.234:8470']\n","WARNING:tensorflow:TPU system grpc://10.116.77.234:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stdout"},{"output_type":"stream","text":["W0611 06:11:49.996057 140614302525312 tpu_strategy_util.py:72] [tpu_strategy_util.py:72] TPU system grpc://10.116.77.234:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.116.77.234:8470\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:11:50.004677 140614302525312 tpu_strategy_util.py:74] [tpu_strategy_util.py:74] Initializing the TPU system: grpc://10.116.77.234:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.538751 140614302525312 tpu_strategy_util.py:109] [tpu_strategy_util.py:109] Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.545741 140614302525312 tpu_strategy_util.py:135] [tpu_strategy_util.py:135] Finished initializing TPU system.\n","W0611 06:12:01.549892 140614302525312 tpu_strategy.py:637] [tpu_strategy.py:637] `tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.553171 140614302525312 tpu_system_metadata.py:159] [tpu_system_metadata.py:159] Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.556586 140614302525312 tpu_system_metadata.py:160] [tpu_system_metadata.py:160] *** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.559770 140614302525312 tpu_system_metadata.py:161] [tpu_system_metadata.py:161] *** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.562867 140614302525312 tpu_system_metadata.py:163] [tpu_system_metadata.py:163] *** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.565802 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.568715 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.571584 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.574580 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.577550 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.580313 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.583082 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.586010 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.588822 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.591976 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.594939 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.597923 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n","I0611 06:12:01.604429 140614302525312 <ipython-input-15-f2d7661ec23b>:122] [<ipython-input-15-f2d7661ec23b>:122] Saving checkpoints at /content/gdrive/My Drive/tmp/cifar\n","I0611 06:12:01.609679 140614302525312 <ipython-input-15-f2d7661ec23b>:131] [<ipython-input-15-f2d7661ec23b>:131] Use TPU at local\n"],"name":"stderr"},{"output_type":"stream","text":["REPLICAS:  8\n","WARNING:tensorflow:TPU system grpc://10.116.77.234:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stdout"},{"output_type":"stream","text":["W0611 06:12:01.622187 140614302525312 tpu_strategy_util.py:72] [tpu_strategy_util.py:72] TPU system grpc://10.116.77.234:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.116.77.234:8470\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:01.625322 140614302525312 tpu_strategy_util.py:74] [tpu_strategy_util.py:74] Initializing the TPU system: grpc://10.116.77.234:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.225932 140614302525312 tpu_strategy_util.py:109] [tpu_strategy_util.py:109] Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.232340 140614302525312 tpu_strategy_util.py:135] [tpu_strategy_util.py:135] Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.237978 140614302525312 tpu_system_metadata.py:159] [tpu_system_metadata.py:159] Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.242556 140614302525312 tpu_system_metadata.py:160] [tpu_system_metadata.py:160] *** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.246431 140614302525312 tpu_system_metadata.py:161] [tpu_system_metadata.py:161] *** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.249040 140614302525312 tpu_system_metadata.py:163] [tpu_system_metadata.py:163] *** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.251558 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.254000 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.256519 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.258945 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.261408 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.264412 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.267463 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.270143 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.272771 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.275192 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.277644 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.279994 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n","I0611 06:12:11.283856 140614302525312 dataset_info.py:361] [dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/cifar10/3.0.2\n","I0611 06:12:11.288350 140614302525312 <ipython-input-15-f2d7661ec23b>:142] [<ipython-input-15-f2d7661ec23b>:142] Steps per epoch 97\n","I0611 06:12:11.290326 140614302525312 <ipython-input-15-f2d7661ec23b>:143] [<ipython-input-15-f2d7661ec23b>:143] Size of the dataset 50000\n","I0611 06:12:11.291980 140614302525312 <ipython-input-15-f2d7661ec23b>:144] [<ipython-input-15-f2d7661ec23b>:144] Train proportion 1.0\n"],"name":"stderr"},{"output_type":"stream","text":["Running on TPU  ['10.116.77.234:8470']\n","WARNING:tensorflow:TPU system grpc://10.116.77.234:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stdout"},{"output_type":"stream","text":["W0611 06:12:11.304185 140614302525312 tpu_strategy_util.py:72] [tpu_strategy_util.py:72] TPU system grpc://10.116.77.234:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.116.77.234:8470\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:11.306744 140614302525312 tpu_strategy_util.py:74] [tpu_strategy_util.py:74] Initializing the TPU system: grpc://10.116.77.234:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.508917 140614302525312 tpu_strategy_util.py:109] [tpu_strategy_util.py:109] Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.516311 140614302525312 tpu_strategy_util.py:135] [tpu_strategy_util.py:135] Finished initializing TPU system.\n","W0611 06:12:23.520535 140614302525312 tpu_strategy.py:637] [tpu_strategy.py:637] `tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.524928 140614302525312 tpu_system_metadata.py:159] [tpu_system_metadata.py:159] Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.528197 140614302525312 tpu_system_metadata.py:160] [tpu_system_metadata.py:160] *** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.532667 140614302525312 tpu_system_metadata.py:161] [tpu_system_metadata.py:161] *** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.535746 140614302525312 tpu_system_metadata.py:163] [tpu_system_metadata.py:163] *** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.539967 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.542954 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.547371 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.550411 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.553094 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.555973 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.558648 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.561433 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.569378 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.576528 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.579466 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 06:12:23.582600 140614302525312 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["REPLICAS:  8\n"],"name":"stdout"},{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-f2d7661ec23b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m   \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-f2d7661ec23b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mdownload_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m       \u001b[0maug_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       validation_percent=1. - FLAGS.train_proportion,)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1222\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10509\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10510\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10511\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10512\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10513\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6899\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6900\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6901\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6902\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: '_EagerConst' is neither a type of a primitive operation nor a name of a function registered in binary running on n-474679f8-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:StridedSlice] name: strided_slice/"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"M68k_LJNKeqS","executionInfo":{"status":"ok","timestamp":1623248728730,"user_tz":-540,"elapsed":291,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"884ff8d3-53af-49ee-c56d-587334d177ae"},"source":["FLAGS.output_dir"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/My Drive/tmp/cifar'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXJPMILeq-rS","executionInfo":{"status":"ok","timestamp":1623240491247,"user_tz":-540,"elapsed":344,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"7e11c957-e2a8-42a2-8e0a-80a25d4ddbf5"},"source":["seeds = tf.random.experimental.stateless_split(\n","      [42, 43], 2)[:, 0]\n","seeds"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1769886085,   86449935], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"DOkeAsz7m1ni","executionInfo":{"status":"ok","timestamp":1623240508250,"user_tz":-540,"elapsed":1461,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["model = wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=FLAGS.l2,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=86449935)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CDdZu7SlOIA","executionInfo":{"status":"ok","timestamp":1623240260635,"user_tz":-540,"elapsed":12370,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"0deed08d-9475-4c7c-87ed-2a7f4e0b09cf"},"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy()\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Running on TPU  ['10.56.50.170:8470']\n","WARNING:tensorflow:TPU system grpc://10.56.50.170:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stdout"},{"output_type":"stream","text":["W0609 12:04:08.147099 140478840584064 tpu_strategy_util.py:72] [tpu_strategy_util.py:72] TPU system grpc://10.56.50.170:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.56.50.170:8470\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:08.152125 140478840584064 tpu_strategy_util.py:74] [tpu_strategy_util.py:74] Initializing the TPU system: grpc://10.56.50.170:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.230001 140478840584064 tpu_strategy_util.py:109] [tpu_strategy_util.py:109] Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.236536 140478840584064 tpu_strategy_util.py:135] [tpu_strategy_util.py:135] Finished initializing TPU system.\n","W0609 12:04:20.244156 140478840584064 tpu_strategy.py:637] [tpu_strategy.py:637] `tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.248939 140478840584064 tpu_system_metadata.py:159] [tpu_system_metadata.py:159] Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.253177 140478840584064 tpu_system_metadata.py:160] [tpu_system_metadata.py:160] *** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.255803 140478840584064 tpu_system_metadata.py:161] [tpu_system_metadata.py:161] *** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.258322 140478840584064 tpu_system_metadata.py:163] [tpu_system_metadata.py:163] *** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.260984 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.263622 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.266195 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.268714 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.271498 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.274281 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.276744 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.279332 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.281800 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.284365 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.286859 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0609 12:04:20.289381 140478840584064 tpu_system_metadata.py:165] [tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["REPLICAS:  8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RjflSLzHmX9w","executionInfo":{"status":"ok","timestamp":1623240266125,"user_tz":-540,"elapsed":1745,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["num_classes = 100\n","model = wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=FLAGS.l2,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=2021)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiKbYEv0iiye","executionInfo":{"status":"ok","timestamp":1623238260039,"user_tz":-540,"elapsed":390,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["def _extract_hyperparameter_dictionary():\n","  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","  hp_keys = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","  hps = {'bn_l2':None, 'input_conv_l2':None, 'group_1_conv_l2':None, 'group_2_conv_l2':None,\n","           'group_3_conv_l2':None, 'dense_kernel_l2':None, 'dense_bias_l2':None}\n","  return hps"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgXtLY0ScWOY","executionInfo":{"status":"ok","timestamp":1623238999875,"user_tz":-540,"elapsed":1469,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":[""],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNECPWqdiGsz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7Gu1y6wiG3G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3sMbNA6iHCo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkFA3MGrb6EM","executionInfo":{"status":"ok","timestamp":1623237333303,"user_tz":-540,"elapsed":313,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}}},"source":["def del_all_flags(FLAGS):\n","    flags_dict = FLAGS._flags()\n","    keys_list = [keys for keys in flags_dict]\n","    \n","    for keys in keys_list:\n","    \tFLAGS.__delattr__(keys)\n","        \n","del_all_flags(flags.FLAGS)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOC6J8OaNyKr","colab":{"base_uri":"https://localhost:8080/","height":188},"executionInfo":{"status":"error","timestamp":1622897960103,"user_tz":-540,"elapsed":1141,"user":{"displayName":"­김덕성 | 서울 산업공학과","photoUrl":"","userId":"15126471116977583172"}},"outputId":"3436d584-9c74-47c5-9ec0-2035cfbc3109"},"source":["import os\n","import time\n","from absl import app\n","#from absl import flags\n","from absl import logging\n","import robustness_metrics as rm\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import uncertainty_baselines as ub\n","import utils  # local file import\n","from tensorboard.plugins.hparams import api as hp\n","\n","\n","def _extract_hyperparameter_dictionary():\n","  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","  hp_keys = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","  hps = {'bn_l2':None, 'input_conv_l2':None, 'group_1_conv_l2':None, 'group_2_conv_l2':None,\n","           'group_3_conv_l2':None, 'dense_kernel_l2':None, 'dense_bias_l2':None}\n","  return hps\n","\n","\n","def main(argv):\n","  fmt = '[%(filename)s:%(lineno)s] %(message)s'\n","  formatter = logging.PythonFormatter(fmt)\n","  logging.get_absl_handler().setFormatter(formatter)\n","  del argv  # unused arg\n","\n","  tf.io.gfile.makedirs(output_dir)\n","  logging.info('Saving checkpoints at %s', output_dir)\n","  tf.random.set_seed(2021)\n","  data_dir = None\n","  if False:\n","    logging.info('Use GPU')\n","    strategy = tf.distribute.MirroredStrategy()\n","  else:\n","    logging.info('Use TPU at %s',\n","                 None if None is not None else 'local')\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=None)\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.TPUStrategy(resolver)\n","\n","  ds_info = tfds.builder('cifar10').info\n","  batch_size = 64 * 8\n","  train_dataset_size = (\n","      ds_info.splits['train'].num_examples * 1.0)\n","  steps_per_epoch = int(train_dataset_size / batch_size)\n","  logging.info('Steps per epoch %s', steps_per_epoch)\n","  logging.info('Size of the dataset %s', ds_info.splits['train'].num_examples)\n","  logging.info('Train proportion %s', 1.0)\n","  steps_per_eval = ds_info.splits['test'].num_examples // batch_size\n","  num_classes = ds_info.features['label'].num_classes\n","\n","  aug_params = {\n","      'augmix': False,\n","      'aug_count': 1,\n","      'augmix_depth': -1,\n","      'augmix_prob_coeff': 0.5,\n","      'augmix_width': 3,\n","  }\n","\n","  # Note that stateless_{fold_in,split} may incur a performance cost, but a\n","  # quick side-by-side test seemed to imply this was minimal.\n","  seeds = tf.random.experimental.stateless_split(\n","      [2021, 2021 + 1], 2)[:, 0]\n","  train_builder = ub.datasets.get(\n","      'cifar10',\n","      data_dir=data_dir,\n","      download_data=False,\n","      split=tfds.Split.TRAIN,\n","      seed=seeds[0],\n","      aug_params=aug_params,\n","      validation_percent=1. - 1.0,)\n","  train_dataset = train_builder.load(batch_size=batch_size)\n","  validation_dataset = None\n","  steps_per_validation = 0\n","  if 1.0 < 1.0:\n","    validation_builder = ub.datasets.get(\n","        'cifar10',\n","        split=tfds.Split.VALIDATION,\n","        validation_percent=1. - 1.0,\n","        data_dir=data_dir)\n","    validation_dataset = validation_builder.load(batch_size=batch_size)\n","    validation_dataset = strategy.experimental_distribute_dataset(\n","        validation_dataset)\n","    steps_per_validation = validation_builder.num_examples // batch_size\n","  clean_test_builder = ub.datasets.get(\n","      'cifar10',\n","      split=tfds.Split.TEST,\n","      data_dir=data_dir)\n","  clean_test_dataset = clean_test_builder.load(batch_size=batch_size)\n","  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","  test_datasets = {\n","      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n","  }\n","  steps_per_epoch = train_builder.num_examples // batch_size\n","  steps_per_eval = clean_test_builder.num_examples // batch_size\n","  num_classes = 100 if 'cifar10' == 'cifar100' else 10\n","  # if -1 > 0:\n","  #   if 'cifar10' == 'cifar100':\n","  #     #data_dir = FLAGS.cifar100_c_path\n","  #   corruption_types, _ = utils.load_corrupted_test_info('cifar10')\n","  #   for corruption_type in corruption_types:\n","  #     for severity in range(1, 6):\n","  #       dataset = ub.datasets.get(\n","  #           'cifar10_corrupted',\n","  #           corruption_type=corruption_type,\n","  #           severity=severity,\n","  #           split=tfds.Split.TEST,\n","  #           data_dir=data_dir).load(batch_size=batch_size)\n","  #       test_datasets[f'{corruption_type}_{severity}'] = (\n","  #           strategy.experimental_distribute_dataset(dataset))\n","\n","  summary_writer = tf.summary.create_file_writer(\n","      os.path.join(output_dir, 'summaries'))\n","\n","  with strategy.scope():\n","    logging.info('Building ResNet model')\n","    model = ub.models.wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=2e-4,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=seeds[1])\n","    logging.info('Model input shape: %s', model.input_shape)\n","    logging.info('Model output shape: %s', model.output_shape)\n","    logging.info('Model number of weights: %s', model.count_params())\n","    # Linearly scale learning rate and the decay epochs by vanilla settings.\n","    base_lr = 0.1 * batch_size / 128\n","    lr_decay_epochs = [(int(start_epoch_str) * 200) // 200\n","                       for start_epoch_str in ['60', '120', '160'] ]\n","    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule(\n","        steps_per_epoch,\n","        base_lr,\n","        decay_ratio=0.2,\n","        decay_epochs=lr_decay_epochs,\n","        warmup_epochs=1)\n","    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n","                                        momentum=1.0 - 0.1,\n","                                        nesterov=True)\n","    metrics = {\n","        'train/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'train/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'train/loss':\n","            tf.keras.metrics.Mean(),\n","        'train/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","        'test/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'test/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'test/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","    }\n","    if validation_dataset:\n","      metrics.update({\n","          'validation/negative_log_likelihood': tf.keras.metrics.Mean(),\n","          'validation/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n","          'validation/ece': rm.metrics.ExpectedCalibrationError(\n","              num_bins=15),\n","      })\n","    if -1 > 0:\n","      corrupt_metrics = {}\n","      for intensity in range(1, 6):\n","        for corruption in corruption_types:\n","          dataset_name = '{0}_{1}'.format(corruption, intensity)\n","          corrupt_metrics['test/nll_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.Mean())\n","          corrupt_metrics['test/accuracy_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.SparseCategoricalAccuracy())\n","          corrupt_metrics['test/ece_{}'.format(dataset_name)] = (\n","              rm.metrics.ExpectedCalibrationError(num_bins=15))\n","\n","    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n","    latest_checkpoint = tf.train.latest_checkpoint(output_dir)\n","    initial_epoch = 0\n","    if latest_checkpoint:\n","      # checkpoint.restore must be within a strategy.scope() so that optimizer\n","      # slot variables are mirrored.\n","      checkpoint.restore(latest_checkpoint)\n","      logging.info('Loaded checkpoint %s', latest_checkpoint)\n","      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n","\n","  @tf.function\n","  def train_step(iterator):\n","    \"\"\"Training StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","\n","      if False and 1 >= 1:\n","        # Index 0 at augmix processing is the unperturbed image.\n","        # We take just 1 augmented image from the returned augmented images.\n","        images = images[:, 1, ...]\n","      with tf.GradientTape() as tape:\n","        logits = model(images, training=True)\n","        if 0 == 0.:\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.sparse_categorical_crossentropy(labels,\n","                                                              logits,\n","                                                              from_logits=True))\n","        else:\n","          one_hot_labels = tf.one_hot(tf.cast(labels, tf.int32), num_classes)\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.categorical_crossentropy(\n","                  one_hot_labels,\n","                  logits,\n","                  from_logits=True,\n","                  label_smoothing= 0 ))\n","        l2_loss = sum(model.losses)\n","        loss = negative_log_likelihood + l2_loss\n","        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n","        scaled_loss = loss / strategy.num_replicas_in_sync\n","\n","      grads = tape.gradient(scaled_loss, model.trainable_variables)\n","      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","      probs = tf.nn.softmax(logits)\n","      metrics['train/ece'].add_batch(probs, label=labels)\n","      metrics['train/loss'].update_state(loss)\n","      metrics['train/negative_log_likelihood'].update_state(\n","          negative_log_likelihood)\n","      metrics['train/accuracy'].update_state(labels, logits)\n","\n","    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  @tf.function\n","  def test_step(iterator, dataset_split, dataset_name, num_steps):\n","    \"\"\"Evaluation StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","      logits = model(images, training=False)\n","      probs = tf.nn.softmax(logits)\n","      negative_log_likelihood = tf.reduce_mean(\n","          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n","\n","      if dataset_name == 'clean':\n","        metrics[f'{dataset_split}/negative_log_likelihood'].update_state(\n","            negative_log_likelihood)\n","        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs)\n","        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels)\n","      else:\n","        corrupt_metrics['test/nll_{}'.format(dataset_name)].update_state(\n","            negative_log_likelihood)\n","        corrupt_metrics['test/accuracy_{}'.format(dataset_name)].update_state(\n","            labels, probs)\n","        corrupt_metrics['test/ece_{}'.format(dataset_name)].add_batch(\n","            probs, label=labels)\n","\n","    for _ in tf.range(tf.cast(num_steps, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n","  metrics.update({'train/ms_per_example': tf.keras.metrics.Mean()})\n","\n","  train_iterator = iter(train_dataset)\n","  start_time = time.time()\n","  tb_callback = None\n","  if False :\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        profile_batch=(100, 102),\n","        log_dir=os.path.join(output_dir, 'logs'))\n","    tb_callback.set_model(model)\n","  for epoch in range(initial_epoch, 200):\n","    logging.info('Starting to run epoch: %s', epoch)\n","    if tb_callback:\n","      tb_callback.on_epoch_begin(epoch)\n","    train_start_time = time.time()\n","    train_step(train_iterator)\n","    ms_per_example = (time.time() - train_start_time) * 1e6 / batch_size\n","    metrics['train/ms_per_example'].update_state(ms_per_example)\n","\n","    current_step = (epoch + 1) * steps_per_epoch\n","    max_steps = steps_per_epoch * 200\n","    time_elapsed = time.time() - start_time\n","    steps_per_sec = float(current_step) / time_elapsed\n","    eta_seconds = (max_steps - current_step) / steps_per_sec\n","    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n","               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n","                   current_step / max_steps,\n","                   epoch + 1,\n","                   200,\n","                   steps_per_sec,\n","                   eta_seconds / 60,\n","                   time_elapsed / 60))\n","    logging.info(message)\n","    if tb_callback:\n","      tb_callback.on_epoch_end(epoch)\n","\n","    if validation_dataset:\n","      validation_iterator = iter(validation_dataset)\n","      test_step(\n","          validation_iterator, 'validation', 'clean', steps_per_validation)\n","    datasets_to_evaluate = {'clean': test_datasets['clean']}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      datasets_to_evaluate = test_datasets\n","    for dataset_name, test_dataset in datasets_to_evaluate.items():\n","      test_iterator = iter(test_dataset)\n","      logging.info('Testing on dataset %s', dataset_name)\n","      logging.info('Starting to run eval at epoch: %s', epoch)\n","      test_start_time = time.time()\n","      test_step(test_iterator, 'test', dataset_name, steps_per_eval)\n","      ms_per_example = (time.time() - test_start_time) * 1e6 / batch_size\n","      metrics['test/ms_per_example'].update_state(ms_per_example)\n","\n","      logging.info('Done with testing on %s', dataset_name)\n","\n","    corrupt_results = {}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n","                                                        corruption_types)\n","\n","    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n","                 metrics['train/loss'].result(),\n","                 metrics['train/accuracy'].result() * 100)\n","    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n","                 metrics['test/negative_log_likelihood'].result(),\n","                 metrics['test/accuracy'].result() * 100)\n","    total_results = {name: metric.result() for name, metric in metrics.items()}\n","    total_results.update(corrupt_results)\n","    # Metrics from Robustness Metrics (like ECE) will return a dict with a\n","    # single key/value, instead of a scalar.\n","    total_results = {\n","        k: (list(v.values())[0] if isinstance(v, dict) else v)\n","        for k, v in total_results.items()\n","    }\n","    with summary_writer.as_default():\n","      for name, result in total_results.items():\n","        tf.summary.scalar(name, result, step=epoch + 1)\n","\n","    for metric in metrics.values():\n","      metric.reset_states()\n","\n","    if (25 > 0 and\n","        (epoch + 1) % 25 == 0):\n","      checkpoint_name = checkpoint.save(\n","          os.path.join(output_dir, 'checkpoint'))\n","      logging.info('Saved checkpoint to %s', checkpoint_name)\n","\n","  final_checkpoint_name = checkpoint.save(\n","      os.path.join(output_dir, 'checkpoint'))\n","  logging.info('Saved last checkpoint to %s', final_checkpoint_name)\n","  with summary_writer.as_default():\n","    hp.hparams({\n","        'base_learning_rate': 0.1,\n","        'one_minus_momentum': 0.1,\n","        'l2': 2e-4,\n","    })\n","\n","\n","if __name__ == '__main__':\n","  app.run(main)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["FATAL Flags parsing error: Unknown command line flag 'f'\n","Pass --helpshort or --helpfull to see help on flags.\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wsLSzzXCLli0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcV54P2ZRF_v"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0q8Mq_PRGJh"},"source":["import os\n","import time\n","from absl import app\n","#from absl import flags\n","from absl import logging\n","import robustness_metrics as rm\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import uncertainty_baselines as ub\n","#import utils  # local file import\n","from tensorboard.plugins.hparams import api as hp\n","\n","output_dir = '/happy'\n","\n","def _extract_hyperparameter_dictionary():\n","  \"\"\"Create the dictionary of hyperparameters from FLAGS.\"\"\"\n","  hp_keys = ('bn_l2', 'input_conv_l2', 'group_1_conv_l2', 'group_2_conv_l2',\n","           'group_3_conv_l2', 'dense_kernel_l2', 'dense_bias_l2')\n","  hps = {'bn_l2':None, 'input_conv_l2':None, 'group_1_conv_l2':None, 'group_2_conv_l2':None,\n","           'group_3_conv_l2':None, 'dense_kernel_l2':None, 'dense_bias_l2':None}\n","  return hps\n","\n","\n","def main(argv):\n","  fmt = '[%(filename)s:%(lineno)s] %(message)s'\n","  print('1')\n","  formatter = logging.PythonFormatter(fmt)\n","  print('2')\n","  logging.get_absl_handler().setFormatter(formatter)\n","  del argv  # unused arg\n","\n","  tf.io.gfile.makedirs(output_dir)\n","  logging.info('Saving checkpoints at %s', output_dir)\n","  tf.random.set_seed(2021)\n","  data_dir = None\n","  if False:\n","    logging.info('Use GPU')\n","    strategy = tf.distribute.MirroredStrategy()\n","  else:\n","    logging.info('Use TPU at %s',\n","                 None if None is not None else 'local')\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=None)\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","    strategy = tf.distribute.TPUStrategy(resolver)\n","\n","  ds_info = tfds.builder('cifar10').info\n","  batch_size = 64 * 8\n","  train_dataset_size = (\n","      ds_info.splits['train'].num_examples * 1.0)\n","  steps_per_epoch = int(train_dataset_size / batch_size)\n","  logging.info('Steps per epoch %s', steps_per_epoch)\n","  logging.info('Size of the dataset %s', ds_info.splits['train'].num_examples)\n","  logging.info('Train proportion %s', 1.0)\n","  steps_per_eval = ds_info.splits['test'].num_examples // batch_size\n","  num_classes = ds_info.features['label'].num_classes\n","\n","  aug_params = {\n","      'augmix': False,\n","      'aug_count': 1,\n","      'augmix_depth': -1,\n","      'augmix_prob_coeff': 0.5,\n","      'augmix_width': 3,\n","  }\n","\n","  # Note that stateless_{fold_in,split} may incur a performance cost, but a\n","  # quick side-by-side test seemed to imply this was minimal.\n","  seeds = tf.random.experimental.stateless_split(\n","      [2021, 2021 + 1], 2)[:, 0]\n","  train_builder = ub.datasets.get(\n","      'cifar10',\n","      data_dir=data_dir,\n","      download_data=False,\n","      split=tfds.Split.TRAIN,\n","      seed=seeds[0],\n","      aug_params=aug_params,\n","      validation_percent=1. - 1.0,)\n","  train_dataset = train_builder.load(batch_size=batch_size)\n","  validation_dataset = None\n","  steps_per_validation = 0\n","  if 1.0 < 1.0:\n","    validation_builder = ub.datasets.get(\n","        'cifar10',\n","        split=tfds.Split.VALIDATION,\n","        validation_percent=1. - 1.0,\n","        data_dir=data_dir)\n","    validation_dataset = validation_builder.load(batch_size=batch_size)\n","    validation_dataset = strategy.experimental_distribute_dataset(\n","        validation_dataset)\n","    steps_per_validation = validation_builder.num_examples // batch_size\n","  clean_test_builder = ub.datasets.get(\n","      'cifar10',\n","      split=tfds.Split.TEST,\n","      data_dir=data_dir)\n","  clean_test_dataset = clean_test_builder.load(batch_size=batch_size)\n","  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","  test_datasets = {\n","      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n","  }\n","  steps_per_epoch = train_builder.num_examples // batch_size\n","  steps_per_eval = clean_test_builder.num_examples // batch_size\n","  num_classes = 100 if 'cifar10' == 'cifar100' else 10\n","  # if -1 > 0:\n","  #   if 'cifar10' == 'cifar100':\n","  #     #data_dir = FLAGS.cifar100_c_path\n","  #   corruption_types, _ = utils.load_corrupted_test_info('cifar10')\n","  #   for corruption_type in corruption_types:\n","  #     for severity in range(1, 6):\n","  #       dataset = ub.datasets.get(\n","  #           'cifar10_corrupted',\n","  #           corruption_type=corruption_type,\n","  #           severity=severity,\n","  #           split=tfds.Split.TEST,\n","  #           data_dir=data_dir).load(batch_size=batch_size)\n","  #       test_datasets[f'{corruption_type}_{severity}'] = (\n","  #           strategy.experimental_distribute_dataset(dataset))\n","\n","  summary_writer = tf.summary.create_file_writer(\n","      os.path.join(output_dir, 'summaries'))\n","\n","  with strategy.scope():\n","    logging.info('Building ResNet model')\n","    model = ub.models.wide_resnet(\n","        input_shape=(32, 32, 3),\n","        depth=28,\n","        width_multiplier=10,\n","        num_classes=num_classes,\n","        l2=2e-4,\n","        hps=_extract_hyperparameter_dictionary(),\n","        seed=seeds[1])\n","    logging.info('Model input shape: %s', model.input_shape)\n","    logging.info('Model output shape: %s', model.output_shape)\n","    logging.info('Model number of weights: %s', model.count_params())\n","    # Linearly scale learning rate and the decay epochs by vanilla settings.\n","    base_lr = 0.1 * batch_size / 128\n","    lr_decay_epochs = [(int(start_epoch_str) * 200) // 200\n","                       for start_epoch_str in ['60', '120', '160'] ]\n","    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule(\n","        steps_per_epoch,\n","        base_lr,\n","        decay_ratio=0.2,\n","        decay_epochs=lr_decay_epochs,\n","        warmup_epochs=1)\n","    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n","                                        momentum=1.0 - 0.1,\n","                                        nesterov=True)\n","    metrics = {\n","        'train/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'train/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'train/loss':\n","            tf.keras.metrics.Mean(),\n","        'train/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","        'test/negative_log_likelihood':\n","            tf.keras.metrics.Mean(),\n","        'test/accuracy':\n","            tf.keras.metrics.SparseCategoricalAccuracy(),\n","        'test/ece':\n","            rm.metrics.ExpectedCalibrationError(num_bins=15),\n","    }\n","    if validation_dataset:\n","      metrics.update({\n","          'validation/negative_log_likelihood': tf.keras.metrics.Mean(),\n","          'validation/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n","          'validation/ece': rm.metrics.ExpectedCalibrationError(\n","              num_bins=15),\n","      })\n","    if -1 > 0:\n","      corrupt_metrics = {}\n","      for intensity in range(1, 6):\n","        for corruption in corruption_types:\n","          dataset_name = '{0}_{1}'.format(corruption, intensity)\n","          corrupt_metrics['test/nll_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.Mean())\n","          corrupt_metrics['test/accuracy_{}'.format(dataset_name)] = (\n","              tf.keras.metrics.SparseCategoricalAccuracy())\n","          corrupt_metrics['test/ece_{}'.format(dataset_name)] = (\n","              rm.metrics.ExpectedCalibrationError(num_bins=15))\n","\n","    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n","    latest_checkpoint = tf.train.latest_checkpoint(output_dir)\n","    initial_epoch = 0\n","    if latest_checkpoint:\n","      # checkpoint.restore must be within a strategy.scope() so that optimizer\n","      # slot variables are mirrored.\n","      checkpoint.restore(latest_checkpoint)\n","      logging.info('Loaded checkpoint %s', latest_checkpoint)\n","      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n","\n","  @tf.function\n","  def train_step(iterator):\n","    \"\"\"Training StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","\n","      if False and 1 >= 1:\n","        # Index 0 at augmix processing is the unperturbed image.\n","        # We take just 1 augmented image from the returned augmented images.\n","        images = images[:, 1, ...]\n","      with tf.GradientTape() as tape:\n","        logits = model(images, training=True)\n","        if 0 == 0.:\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.sparse_categorical_crossentropy(labels,\n","                                                              logits,\n","                                                              from_logits=True))\n","        else:\n","          one_hot_labels = tf.one_hot(tf.cast(labels, tf.int32), num_classes)\n","          negative_log_likelihood = tf.reduce_mean(\n","              tf.keras.losses.categorical_crossentropy(\n","                  one_hot_labels,\n","                  logits,\n","                  from_logits=True,\n","                  label_smoothing= 0 ))\n","        l2_loss = sum(model.losses)\n","        loss = negative_log_likelihood + l2_loss\n","        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n","        scaled_loss = loss / strategy.num_replicas_in_sync\n","\n","      grads = tape.gradient(scaled_loss, model.trainable_variables)\n","      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","      probs = tf.nn.softmax(logits)\n","      metrics['train/ece'].add_batch(probs, label=labels)\n","      metrics['train/loss'].update_state(loss)\n","      metrics['train/negative_log_likelihood'].update_state(\n","          negative_log_likelihood)\n","      metrics['train/accuracy'].update_state(labels, logits)\n","\n","    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  @tf.function\n","  def test_step(iterator, dataset_split, dataset_name, num_steps):\n","    \"\"\"Evaluation StepFn.\"\"\"\n","    def step_fn(inputs):\n","      \"\"\"Per-Replica StepFn.\"\"\"\n","      images = inputs['features']\n","      labels = inputs['labels']\n","      logits = model(images, training=False)\n","      probs = tf.nn.softmax(logits)\n","      negative_log_likelihood = tf.reduce_mean(\n","          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n","\n","      if dataset_name == 'clean':\n","        metrics[f'{dataset_split}/negative_log_likelihood'].update_state(\n","            negative_log_likelihood)\n","        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs)\n","        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels)\n","      else:\n","        corrupt_metrics['test/nll_{}'.format(dataset_name)].update_state(\n","            negative_log_likelihood)\n","        corrupt_metrics['test/accuracy_{}'.format(dataset_name)].update_state(\n","            labels, probs)\n","        corrupt_metrics['test/ece_{}'.format(dataset_name)].add_batch(\n","            probs, label=labels)\n","\n","    for _ in tf.range(tf.cast(num_steps, tf.int32)):\n","      strategy.run(step_fn, args=(next(iterator),))\n","\n","  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n","  metrics.update({'train/ms_per_example': tf.keras.metrics.Mean()})\n","\n","  train_iterator = iter(train_dataset)\n","  start_time = time.time()\n","  tb_callback = None\n","  if False :\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        profile_batch=(100, 102),\n","        log_dir=os.path.join(output_dir, 'logs'))\n","    tb_callback.set_model(model)\n","  for epoch in range(initial_epoch, 200):\n","    logging.info('Starting to run epoch: %s', epoch)\n","    if tb_callback:\n","      tb_callback.on_epoch_begin(epoch)\n","    train_start_time = time.time()\n","    train_step(train_iterator)\n","    ms_per_example = (time.time() - train_start_time) * 1e6 / batch_size\n","    metrics['train/ms_per_example'].update_state(ms_per_example)\n","\n","    current_step = (epoch + 1) * steps_per_epoch\n","    max_steps = steps_per_epoch * 200\n","    time_elapsed = time.time() - start_time\n","    steps_per_sec = float(current_step) / time_elapsed\n","    eta_seconds = (max_steps - current_step) / steps_per_sec\n","    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n","               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n","                   current_step / max_steps,\n","                   epoch + 1,\n","                   200,\n","                   steps_per_sec,\n","                   eta_seconds / 60,\n","                   time_elapsed / 60))\n","    logging.info(message)\n","    if tb_callback:\n","      tb_callback.on_epoch_end(epoch)\n","\n","    if validation_dataset:\n","      validation_iterator = iter(validation_dataset)\n","      test_step(\n","          validation_iterator, 'validation', 'clean', steps_per_validation)\n","    datasets_to_evaluate = {'clean': test_datasets['clean']}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      datasets_to_evaluate = test_datasets\n","    for dataset_name, test_dataset in datasets_to_evaluate.items():\n","      test_iterator = iter(test_dataset)\n","      logging.info('Testing on dataset %s', dataset_name)\n","      logging.info('Starting to run eval at epoch: %s', epoch)\n","      test_start_time = time.time()\n","      test_step(test_iterator, 'test', dataset_name, steps_per_eval)\n","      ms_per_example = (time.time() - test_start_time) * 1e6 / batch_size\n","      metrics['test/ms_per_example'].update_state(ms_per_example)\n","\n","      logging.info('Done with testing on %s', dataset_name)\n","\n","    corrupt_results = {}\n","    if (-1 > 0 and\n","        (epoch + 1) % -1 == 0):\n","      corrupt_results = aggregate_corrupt_metrics(corrupt_metrics,\n","                                                        corruption_types)\n","\n","    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n","                 metrics['train/loss'].result(),\n","                 metrics['train/accuracy'].result() * 100)\n","    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n","                 metrics['test/negative_log_likelihood'].result(),\n","                 metrics['test/accuracy'].result() * 100)\n","    total_results = {name: metric.result() for name, metric in metrics.items()}\n","    total_results.update(corrupt_results)\n","    # Metrics from Robustness Metrics (like ECE) will return a dict with a\n","    # single key/value, instead of a scalar.\n","    total_results = {\n","        k: (list(v.values())[0] if isinstance(v, dict) else v)\n","        for k, v in total_results.items()\n","    }\n","    with summary_writer.as_default():\n","      for name, result in total_results.items():\n","        tf.summary.scalar(name, result, step=epoch + 1)\n","\n","    for metric in metrics.values():\n","      metric.reset_states()\n","\n","    if (25 > 0 and\n","        (epoch + 1) % 25 == 0):\n","      checkpoint_name = checkpoint.save(\n","          os.path.join(output_dir, 'checkpoint'))\n","      logging.info('Saved checkpoint to %s', checkpoint_name)\n","\n","  final_checkpoint_name = checkpoint.save(\n","      os.path.join(output_dir, 'checkpoint'))\n","  logging.info('Saved last checkpoint to %s', final_checkpoint_name)\n","  with summary_writer.as_default():\n","    hp.hparams({\n","        'base_learning_rate': 0.1,\n","        'one_minus_momentum': 0.1,\n","        'l2': 2e-4,\n","    })\n","\n","\n","if __name__ == '__main__':\n","  app.run(main)"],"execution_count":null,"outputs":[]}]}